
@misc{gu_mamba_2023,
	title = {Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
	shorttitle = {Mamba},
	url = {http://arxiv.org/abs/2312.00752},
	doi = {10.48550/arXiv.2312.00752},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	urldate = {2023-12-11},
	publisher = {arXiv},
	author = {Gu, Albert and Dao, Tri},
	month = dec,
	year = {2023},
	note = {arXiv:2312.00752 [cs]},
}

@article{lake_building_2017,
	title = {Building machines that learn and think like people},
	volume = {40},
	issn = {1469-1825},
	doi = {10.1017/S0140525X16001837},
	abstract = {Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
	language = {eng},
	journal = {The Behavioral and Brain Sciences},
	author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
	month = jan,
	year = {2017},
	pmid = {27881212},
	pages = {e253},
}

@inproceedings{higgins_beta-vae_2016,
	title = {beta-{VAE}: {Learning} {Basic} {Visual} {Concepts} with a {Constrained} {Variational} {Framework}},
	shorttitle = {beta-{VAE}},
	url = {https://openreview.net/forum?id=Sy2fzU9gl},
	abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned beta {\textgreater} 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.},
	language = {en},
	urldate = {2023-12-08},
	author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
	month = nov,
	year = {2016},
}

@misc{li_learning_2021,
	title = {Learning {Disentangled} {Representations} for {Time} {Series}},
	url = {http://arxiv.org/abs/2105.08179},
	doi = {10.48550/arXiv.2105.08179},
	abstract = {Time-series representation learning is a fundamental task for time-series analysis. While significant progress has been made to achieve accurate representations for downstream applications, the learned representations often lack interpretability and do not expose semantic meanings. Different from previous efforts on the entangled feature space, we aim to extract the semantic-rich temporal correlations in the latent interpretable factorized representation of the data. Motivated by the success of disentangled representation learning in computer vision, we study the possibility of learning semantic-rich time-series representations, which remains unexplored due to three main challenges: 1) sequential data structure introduces complex temporal correlations and makes the latent representations hard to interpret, 2) sequential models suffer from KL vanishing problem, and 3) interpretable semantic concepts for time-series often rely on multiple factors instead of individuals. To bridge the gap, we propose Disentangle Time Series (DTS), a novel disentanglement enhancement framework for sequential data. Specifically, to generate hierarchical semantic concepts as the interpretable and disentangled representation of time-series, DTS introduces multi-level disentanglement strategies by covering both individual latent factors and group semantic segments. We further theoretically show how to alleviate the KL vanishing problem: DTS introduces a mutual information maximization term, while preserving a heavier penalty on the total correlation and the dimension-wise KL to keep the disentanglement property. Experimental results on various real-world benchmark datasets demonstrate that the representations learned by DTS achieve superior performance in downstream applications, with high interpretability of semantic concepts.},
	urldate = {2023-12-08},
	publisher = {arXiv},
	author = {Li, Yuening and Chen, Zhengzhang and Zha, Daochen and Du, Mengnan and Zhang, Denghui and Chen, Haifeng and Hu, Xia},
	month = may,
	year = {2021},
	note = {arXiv:2105.08179 [cs]},
}

@misc{godahewa_monash_2021,
	title = {Monash {Time} {Series} {Forecasting} {Archive}},
	url = {http://arxiv.org/abs/2105.06643},
	doi = {10.48550/arXiv.2105.06643},
	abstract = {Many businesses and industries nowadays rely on large quantities of time series data making time series forecasting an important research area. Global forecasting models that are trained across sets of time series have shown a huge potential in providing accurate forecasts compared with the traditional univariate forecasting models that work on isolated series. However, there are currently no comprehensive time series archives for forecasting that contain datasets of time series from similar sources available for the research community to evaluate the performance of new global forecasting algorithms over a wide variety of datasets. In this paper, we present such a comprehensive time series forecasting archive containing 20 publicly available time series datasets from varied domains, with different characteristics in terms of frequency, series lengths, and inclusion of missing values. We also characterise the datasets, and identify similarities and differences among them, by conducting a feature analysis. Furthermore, we present the performance of a set of standard baseline forecasting methods over all datasets across eight error metrics, for the benefit of researchers using the archive to benchmark their forecasting algorithms.},
	urldate = {2023-12-05},
	publisher = {arXiv},
	author = {Godahewa, Rakshitha and Bergmeir, Christoph and Webb, Geoffrey I. and Hyndman, Rob J. and Montero-Manso, Pablo},
	month = may,
	year = {2021},
	note = {arXiv:2105.06643 [cs, stat]},
}

@article{makridakis_m4_2020,
	series = {M4 {Competition}},
	title = {The {M4} {Competition}: 100,000 time series and 61 forecasting methods},
	volume = {36},
	issn = {0169-2070},
	shorttitle = {The {M4} {Competition}},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207019301128},
	doi = {10.1016/j.ijforecast.2019.04.014},
	abstract = {The M4 Competition follows on from the three previous M competitions, the purpose of which was to learn from empirical evidence both how to improve the forecasting accuracy and how such learning could be used to advance the theory and practice of forecasting. The aim of M4 was to replicate and extend the three previous competitions by: (a) significantly increasing the number of series, (b) expanding the number of forecasting methods, and (c) including prediction intervals in the evaluation process as well as point forecasts. This paper covers all aspects of M4 in detail, including its organization and running, the presentation of its results, the top-performing methods overall and by categories, its major findings and their implications, and the computational requirements of the various methods. Finally, it summarizes its main conclusions and states the expectation that its series will become a testing ground for the evaluation of new methods and the improvement of the practice of forecasting, while also suggesting some ways forward for the field.},
	number = {1},
	urldate = {2023-12-05},
	journal = {International Journal of Forecasting},
	author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
	month = jan,
	year = {2020},
	pages = {54--74},
}

@misc{ho_classifier-free_2022,
	title = {Classifier-{Free} {Diffusion} {Guidance}},
	url = {http://arxiv.org/abs/2207.12598},
	doi = {10.48550/arXiv.2207.12598},
	abstract = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
	urldate = {2023-12-03},
	publisher = {arXiv},
	author = {Ho, Jonathan and Salimans, Tim},
	month = jul,
	year = {2022},
	note = {arXiv:2207.12598 [cs]},
}

@article{koochali_random_2022,
	title = {Random {Noise} vs. {State}-of-the-{Art} {Probabilistic} {Forecasting} {Methods}: {A} {Case} {Study} on {CRPS}-{Sum} {Discrimination} {Ability}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	shorttitle = {Random {Noise} vs. {State}-of-the-{Art} {Probabilistic} {Forecasting} {Methods}},
	url = {https://www.mdpi.com/2076-3417/12/10/5104},
	doi = {10.3390/app12105104},
	abstract = {The recent developments in the machine-learning domain have enabled the development of complex multivariate probabilistic forecasting models. To evaluate the predictive power of these complex methods, it is pivotal to have a precise evaluation method to gauge the performance and predictability power of these complex methods. To do so, several evaluation metrics have been proposed in the past (such as the energy score, Dawid–Sebastiani score, and variogram score); however, these cannot reliably measure the performance of a probabilistic forecaster. Recently, CRPS-Sum has gained a lot of prominence as a reliable metric for multivariate probabilistic forecasting. This paper presents a systematic evaluation of CRPS-Sum to understand its discrimination ability. We show that the statistical properties of target data affect the discrimination ability of CRPS-Sum. Furthermore, we highlight that CRPS-Sum calculation overlooks the performance of the model on each dimension. These flaws can lead us to an incorrect assessment of model performance. Finally, with experiments on real-world datasets, we demonstrate that the shortcomings of CRPS-Sum provide a misleading indication of the probabilistic forecasting performance method. We illustrate that it is easily possible to have a better CRPS-Sum for a dummy model, which looks like random noise, in comparison to the state-of-the-art method.},
	language = {en},
	number = {10},
	urldate = {2023-12-01},
	journal = {Applied Sciences},
	author = {Koochali, Alireza and Schichtel, Peter and Dengel, Andreas and Ahmed, Sheraz},
	month = jan,
	year = {2022},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {5104},
}

@misc{farrokhabadi_day-ahead_2020,
	title = {Day-{Ahead} {Electricity} {Demand} {Forecasting}: {Post}-{COVID} {Paradigm}},
	shorttitle = {Day-{Ahead} {Electricity} {Demand} {Forecasting}},
	url = {https://ieee-dataport.org/competitions/day-ahead-electricity-demand-forecasting-post-covid-paradigm},
	abstract = {Leaderboard (numbers are kW MAE):Teams with more than 5 missing submissions are eliminated from the leaderboard.},
	language = {en},
	urldate = {2023-11-30},
	publisher = {IEEE},
	author = {Farrokhabadi, Mostafa},
	month = nov,
	year = {2020},
}

@article{miller_building_2020,
	title = {The {Building} {Data} {Genome} {Project} 2, energy meter data from the {ASHRAE} {Great} {Energy} {Predictor} {III} competition},
	volume = {7},
	issn = {2052-4463},
	url = {http://arxiv.org/abs/2006.02273},
	doi = {10.1038/s41597-020-00712-x},
	abstract = {This paper describes an open data set of 3,053 energy meters from 1,636 non-residential buildings with a range of two full years (2016 and 2017) at an hourly frequency (17,544 measurements per meter resulting in approximately 53.6 million measurements). These meters were collected from 19 sites across North America and Europe, with one or more meters per building measuring whole building electrical, heating and cooling water, steam, and solar energy as well as water and irrigation meters. Part of these data were used in the Great Energy Predictor III (GEPIII) competition hosted by the ASHRAE organization in October-December 2019. GEPIII was a machine learning competition for long-term prediction with an application to measurement and verification. This paper describes the process of data collection, cleaning, and convergence of time-series meter data, the meta-data about the buildings, and complementary weather data. This data set can be used for further prediction benchmarking and prototyping as well as anomaly detection, energy analysis, and building type classification.},
	number = {1},
	urldate = {2023-11-30},
	journal = {Scientific Data},
	author = {Miller, Clayton and Kathirgamanathan, Anjukan and Picchetti, Bianca and Arjunan, Pandarasamy and Park, June Young and Nagy, Zoltan and Raftery, Paul and Hobson, Brodie W. and Shi, Zixiao and Meggers, Forrest},
	month = oct,
	year = {2020},
	note = {arXiv:2006.02273 [stat]},
	pages = {368},
}

@article{hong_probabilistic_2016,
	title = {Probabilistic energy forecasting: {Global} {Energy} {Forecasting} {Competition} 2014 and beyond},
	volume = {32},
	issn = {0169-2070},
	shorttitle = {Probabilistic energy forecasting},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207016000133},
	doi = {10.1016/j.ijforecast.2016.02.001},
	abstract = {The energy industry has been going through a significant modernization process over the last decade. Its infrastructure is being upgraded rapidly. The supply, demand and prices are becoming more volatile and less predictable than ever before. Even its business model is being challenged fundamentally. In this competitive and dynamic environment, many decision-making processes rely on probabilistic forecasts to quantify the uncertain future. Although most of the papers in the energy forecasting literature focus on point or single-valued forecasts, the research interest in probabilistic energy forecasting research has taken off rapidly in recent years. In this paper, we summarize the recent research progress on probabilistic energy forecasting. A major portion of the paper is devoted to introducing the Global Energy Forecasting Competition 2014 (GEFCom2014), a probabilistic energy forecasting competition with four tracks on load, price, wind and solar forecasting, which attracted 581 participants from 61 countries. We conclude the paper with 12 predictions for the next decade of energy forecasting.},
	number = {3},
	urldate = {2023-11-30},
	journal = {International Journal of Forecasting},
	author = {Hong, Tao and Pinson, Pierre and Fan, Shu and Zareipour, Hamidreza and Troccoli, Alberto and Hyndman, Rob J.},
	month = jul,
	year = {2016},
	pages = {896--913},
}

@article{li_learning_2023,
	title = {Learning {Robust} {Deep} {State} {Space} for {Unsupervised} {Anomaly} {Detection} in {Contaminated} {Time}-{Series}},
	volume = {35},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/document/9773982},
	doi = {10.1109/TKDE.2022.3171562},
	abstract = {Anomalies are ubiquitous in real-world time-series data which call for effective and timely detection, especially in an unsupervised setting for labeling cost saving. In this paper, we develop an unsupervised density reconstruction model for multi-dimensional time-series anomaly detection. In particular, it directly handles an important realistic setting that the detection is achieved towards raw time-series contaminated with noise for training, in contrast to most existing anomaly detection works that assume the training data is in general clean i.e., not contaminated with anomaly. It extends recent advancements in deep generative models and state space models to achieve robust anomaly detection. Our approach comprises of a novel state space based generative model, a filtering based inference model, together with a carefully-designated emission model based on robust statistics theory. Extensive experimental results are conducted to show that our approach can adapt to complex patterns even given severely contaminated training data. We also develop visualization techniques to help better understand the behavior of the anomaly detection models. Empirical results show that our method outperforms state-of-the-arts on both synthetic and real-world datasets.},
	number = {6},
	urldate = {2023-11-30},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Li, Longyuan and Yan, Junchi and Wen, Qingsong and Jin, Yaohui and Yang, Xiaokang},
	month = jun,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	pages = {6058--6072},
}

@incollection{huber_robust_2011,
	address = {Berlin, Heidelberg},
	title = {Robust {Statistics}},
	isbn = {978-3-642-04898-2},
	url = {https://doi.org/10.1007/978-3-642-04898-2_594},
	language = {en},
	urldate = {2023-11-30},
	booktitle = {International {Encyclopedia} of {Statistical} {Science}},
	publisher = {Springer},
	author = {Huber, Peter J.},
	editor = {Lovric, Miodrag},
	year = {2011},
	doi = {10.1007/978-3-642-04898-2_594},
	pages = {1248--1251},
}

@article{wagner_ptb-xl_2020,
	title = {{PTB}-{XL}, a large publicly available electrocardiography dataset},
	volume = {7},
	copyright = {2020 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-020-0495-6},
	doi = {10.1038/s41597-020-0495-6},
	abstract = {Electrocardiography (ECG) is a key non-invasive diagnostic tool for cardiovascular diseases which is increasingly supported by algorithms based on machine learning. Major obstacles for the development of automatic ECG interpretation algorithms are both the lack of public datasets and well-defined benchmarking procedures to allow comparison s of different algorithms. To address these issues, we put forward PTB-XL, the to-date largest freely accessible clinical 12-lead ECG-waveform dataset comprising 21837 records from 18885 patients of 10 seconds length. The ECG-waveform data was annotated by up to two cardiologists as a multi-label dataset, where diagnostic labels were further aggregated into super and subclasses. The dataset covers a broad range of diagnostic classes including, in particular, a large fraction of healthy records. The combination with additional metadata on demographics, additional diagnostic statements, diagnosis likelihoods, manually annotated signal properties as well as suggested folds for splitting training and test sets turns the dataset into a rich resource for the development and the evaluation of automatic ECG interpretation algorithms.},
	language = {en},
	number = {1},
	urldate = {2023-11-29},
	journal = {Scientific Data},
	author = {Wagner, Patrick and Strodthoff, Nils and Bousseljot, Ralf-Dieter and Kreiseler, Dieter and Lunze, Fatima I. and Samek, Wojciech and Schaeffter, Tobias},
	month = may,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {154},
}

@inproceedings{gu_hippo_2020,
	title = {{HiPPO}: {Recurrent} {Memory} with {Optimal} {Polynomial} {Projections}},
	volume = {33},
	shorttitle = {{HiPPO}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/102f0bb6efb3a6128a3c750dd16729be-Abstract.html},
	abstract = {A central problem in learning from sequential data is representing cumulative
history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3\%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40\% accuracy.},
	urldate = {2023-11-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and Ré, Christopher},
	year = {2020},
	keywords = {⛔ No DOI found},
	pages = {1474--1487},
}

@misc{gu_how_2022,
	title = {How to {Train} {Your} {HiPPO}: {State} {Space} {Models} with {Generalized} {Orthogonal} {Basis} {Projections}},
	shorttitle = {How to {Train} {Your} {HiPPO}},
	url = {http://arxiv.org/abs/2206.12037},
	doi = {10.48550/arXiv.2206.12037},
	abstract = {Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86\% on the Long Range Arena benchmark, with 96\% on the most difficult Path-X task.},
	urldate = {2023-11-29},
	publisher = {arXiv},
	author = {Gu, Albert and Johnson, Isys and Timalsina, Aman and Rudra, Atri and Ré, Christopher},
	month = aug,
	year = {2022},
	note = {arXiv:2206.12037 [cs]},
}

@inproceedings{dhariwal_diffusion_2021,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	volume = {34},
	url = {https://papers.nips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html},
	urldate = {2023-10-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dhariwal, Prafulla and Nichol, Alexander},
	year = {2021},
	keywords = {⛔ No DOI found},
	pages = {8780--8794},
}

@inproceedings{de_bezenac_normalizing_2020,
	title = {Normalizing {Kalman} {Filters} for {Multivariate} {Time} {Series} {Analysis}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1f47cef5e38c952f94c5d61726027439-Abstract.html},
	abstract = {This paper tackles the modelling of large, complex and multivariate time series panels in a probabilistic setting. To this extent, we present a novel approach reconciling classical state space models with deep learning methods. By augmenting state space models with normalizing flows, we mitigate imprecisions stemming from idealized assumptions in state space models. The resulting model is highly flexible while still retaining many of the attractive properties of state space models, e.g., uncertainty and observation errors are properly accounted for, inference is tractable, sampling is efficient, good generalization performance is observed, even in low data regimes. We demonstrate competitiveness against state-of-the-art deep learning methods on the tasks of forecasting real world data and handling varying levels of missing data.},
	urldate = {2023-10-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {de Bézenac, Emmanuel and Rangapuram, Syama Sundar and Benidis, Konstantinos and Bohlke-Schneider, Michael and Kurle, Richard and Stella, Lorenzo and Hasson, Hilaf and Gallinari, Patrick and Januschowski, Tim},
	year = {2020},
	keywords = {⛔ No DOI found},
	pages = {2995--3007},
}

@inproceedings{vahdat_nvae_2020,
	title = {{NVAE}: {A} {Deep} {Hierarchical} {Variational} {Autoencoder}},
	volume = {33},
	shorttitle = {{NVAE}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/e3b21256183cf7c2c7a66be163579d37-Abstract.html},
	abstract = {Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256x256 pixels. The source code is publicly available.},
	urldate = {2023-11-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vahdat, Arash and Kautz, Jan},
	year = {2020},
	keywords = {⛔ No DOI found},
	pages = {19667--19679},
}

@inproceedings{austin_structured_2021,
	title = {Structured {Denoising} {Diffusion} {Models} in {Discrete} {State}-{Spaces}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/958c530554f78bcd8e97125b70e6973d-Abstract.html},
	abstract = {Denoising diffusion probabilistic models (DDPMs) [Ho et al. 2021] have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. [2021], by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss.  For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.},
	urldate = {2023-11-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and van den Berg, Rianne},
	year = {2021},
	keywords = {image, ⛔ No DOI found},
	pages = {17981--17993},
}

@inproceedings{dhariwal_diffusion_2021-1,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	volume = {34},
	url = {https://papers.nips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html},
	urldate = {2023-11-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dhariwal, Prafulla and Nichol, Alexander},
	year = {2021},
	keywords = {image, ⛔ No DOI found},
	pages = {8780--8794},
}

@inproceedings{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
	urldate = {2023-09-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	year = {2020},
	keywords = {diffusion, ⛔ No DOI found},
	pages = {6840--6851},
}

@inproceedings{rasul_multivariate_2021,
	title = {Multivariate {Probabilistic} {Time} {Series} {Forecasting} via {Conditioned} {Normalizing} {Flows}},
	url = {https://openreview.net/forum?id=WiGQBFuVRv},
	booktitle = {International {Conference} on {Learning} {Representations}, 2021},
	publisher = {OpenReview.net},
	author = {Rasul, Kashif and Sheikh, Abdul-Saboor and Schuster, Ingmar and Bergmann, Urs M. and Vollgraf, Roland},
	year = {2021},
	keywords = {Normalizing-Flow, Transformer, Transformer-MAF, ⛔ No DOI found},
}

@inproceedings{zhang_diffusion_2021,
	title = {Diffusion {Normalizing} {Flow}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/876f1f9954de0aa402d91bb988d12cd4-Abstract.html},
	abstract = {We present a novel generative modeling method called diffusion normalizing flow based on stochastic differential equations (SDEs). The algorithm consists of two neural SDEs: a forward SDE that gradually adds noise to the data to transform the data into Gaussian random noise, and a backward SDE that gradually removes the noise to sample from the data distribution. By jointly training the two neural SDEs to minimize a common cost function that quantifies the difference between the two, the backward SDE converges to a diffusion process the starts with a Gaussian distribution and ends with the desired data distribution. Our method is closely related to normalizing flow and diffusion probabilistic models, and can be viewed as a combination of the two. Compared with normalizing flow, diffusion normalizing flow is able to learn distributions with sharp boundaries. Compared with diffusion probabilistic models, diffusion normalizing flow requires fewer discretization steps and thus has better sampling efficiency. Our algorithm demonstrates competitive performance in both high-dimension data density estimation and image generation tasks.},
	urldate = {2023-09-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Qinsheng and Chen, Yongxin},
	year = {2021},
	keywords = {Diffusion, NF, ⛔ No DOI found},
	pages = {16280--16291},
}

@article{li_generative_2022,
	title = {Generative {Time} {Series} {Forecasting} with {Diffusion}, {Denoise}, and {Disentanglement}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/91a85f3fb8f570e6be52b333b5ab017a-Abstract-Conference.html},
	language = {en},
	urldate = {2023-10-26},
	journal = {Advances in Neural Information Processing Systems},
	author = {Li, Yan and Lu, Xinjiang and Wang, Yaqing and Dou, Dejing},
	month = dec,
	year = {2022},
	keywords = {⛔ No DOI found},
	pages = {23009--23022},
}

@article{noy_experimental_2023,
	title = {Experimental {Evidence} on the {Productivity} {Effects} of {Generative} {Artificial} {Intelligence}},
	abstract = {We examine the productivity effects of a generative artificial intelligence technology—the assistive chatbot ChatGPT—in the context of mid-level professional writing tasks. In a preregistered online experiment, we assign occupation-specific, incentivized writing tasks to 444 college-educated professionals, and randomly expose half of them to ChatGPT. Our results show that ChatGPT substantially raises average productivity: time taken decreases by 0.8 SDs and output quality rises by 0.4 SDs. Inequality between workers decreases, as ChatGPT compresses the productivity distribution by benefiting low-ability workers more. ChatGPT mostly substitutes for worker effort rather than complementing worker skills, and restructures tasks towards idea-generation and editing and away from rough-drafting. Exposure to ChatGPT increases job satisfaction and self-efficacy and heightens both concern and excitement about automation technologies.},
	language = {en},
	author = {Noy, Shakked and Zhang, Whitney},
	month = mar,
	year = {2023},
	keywords = {⛔ No DOI found},
}

@article{wang_learning_nodate,
	title = {Learning {Latent} {Seasonal}-{Trend} {Representations} for {Time} {Series} {Forecasting}},
	abstract = {Forecasting complex time series is ubiquitous and vital in a range of applications but challenging. Recent advances endeavor to achieve progress by incorporating various deep learning techniques (e.g., RNN and Transformer) into sequential models. However, clear patterns are still hard to extract since time series are often composed of several intricately entangled components. Motivated by the success of disentangled variational autoencoder in computer vision and classical time series decomposition, we plan to infer a couple of representations that depict seasonal and trend components of time series. To achieve this goal, we propose LaST, which, based on variational inference, aims to disentangle the seasonal-trend representations in the latent space. Furthermore, LaST supervises and disassociates representations from the perspectives of themselves and input reconstruction, and introduces a series of auxiliary objectives. Extensive experiments prove that LaST achieves state-of-the-art performance on time series forecasting task against the most advanced representation learning and end-to-end forecasting models. For reproducibility, our implementation is publicly available on Github1.},
	language = {en},
	author = {Wang, Zhiyuan and Xu, Xovee and Zhang, Weifeng and Trajcevski, Goce and Zhong, Ting and Zhou, Fan},
}

@article{watanabe_information_1960,
	title = {Information {Theoretical} {Analysis} of {Multivariate} {Correlation}},
	volume = {4},
	issn = {0018-8646},
	url = {https://ieeexplore.ieee.org/document/5392532},
	doi = {10.1147/rd.41.0066},
	abstract = {A set λ of stochastic variables, y1 ,y2, …, yn, is grouped into subsets, µ1, µ2, ..., µk. The correlation existing in λ with respect to the µ's is adequately expressed by an equation where S(ν) is the entropy function defined with reference to the variables y in subset ν. For a given λ, C becomes maximum when each µi consists of only one variable, (n = k). The value C is then called the total correlation in λ, Ctot(λ). The present paper gives various theorems, according to which Ctot(λ) can be decomposed in terms of the partial correlations existing in subsets of λ, and of quantities derivable therefrom. The information-theoretical meaning of each decomposition is carefully explained. As illustrations, two problems are discussed at the end of the paper: (1) redundancy in geometrical figures in pattern recognition, and (2) randomization effect of shuffling cards marked “zero” or “one.”},
	number = {1},
	urldate = {2023-11-28},
	journal = {IBM Journal of Research and Development},
	author = {Watanabe, Satosi},
	month = jan,
	year = {1960},
	note = {Conference Name: IBM Journal of Research and Development},
	pages = {66--82},
}

@article{galjaard_supervised_nodate,
	title = {Supervised {Few}-{Shot} {Learning}: {Recent} {Advancements} in {Optimization} {Based} {Techniques}},
	abstract = {Recently a signiﬁcant amount of research interest has been put into few-shot learning (FSL) and personalized learning. The predominant body of work has focused on metalearning approaches, where the goal is to ‘learn to learn’. Additionally, others have ventured into related directions, such as multi-task learning and ﬁne-tuning. As this area rapidly develops, a comprehensive overview of the SOTA is valuable for future work. Although prior surveys exist in different directions, an overview is missing that compares and relates different gradientbased few-shot learners.},
	language = {en},
	author = {Galjaard, Jeroen Martijn},
	keywords = {⛔ No DOI found},
}

@article{zhu_defenses_nodate,
	title = {Defenses {Against} {Poisoning} {Attacks} in {Federated} {Learning}: {A} {Survey}},
	abstract = {As an emerging distributed machine learning paradigm, the security and privacy of federated learning have become huge concerns in the past few years. According to previous studies, federated learning is susceptible to poisoning attacks that can degrade the performance of the global model. Despite previous surveys discussing both defenses and attacks against federated learning, an overview is lacking that compares the methods and experiments of defenses against model poisoning attacks or data poisoning attacks .},
	language = {en},
	author = {Zhu, Chaoyi},
	keywords = {⛔ No DOI found},
}

@article{anderson_reverse-time_1982,
	title = {Reverse-time diffusion equation models},
	volume = {12},
	issn = {0304-4149},
	url = {https://www.sciencedirect.com/science/article/pii/0304414982900515},
	doi = {10.1016/0304-4149(82)90051-5},
	abstract = {Reverse-time stochastic diffusion equation models are defined and it is shown how most processes defined via a forward-time or conventional diffusion equation model have an associated reverse-time model.},
	number = {3},
	urldate = {2023-10-20},
	journal = {Stochastic Processes and their Applications},
	author = {Anderson, Brian D. O.},
	month = may,
	year = {1982},
	pages = {313--326},
}

@article{winkler_scoring_1996,
	title = {Scoring rules and the evaluation of probabilities},
	volume = {5},
	issn = {1863-8260},
	url = {https://doi.org/10.1007/BF02562681},
	doi = {10.1007/BF02562681},
	abstract = {In Bayesian inference and decision analysis, inferences and predictions are inherently probabilistic in nature. Scoring rules, which involve the computation of a score based on probability forecasts and what actually occurs, can be used to evaluate probabilities and to provide appropriate incentives for “good” probabilities. This paper review scoring rules and some related measures for evaluating probabilities, including decompositions of scoring rules and attributes of “goodness” of probabilites, comparability of scores, and the design of scoring rules for specific inferential and decision-making problems},
	language = {en},
	number = {1},
	urldate = {2023-10-09},
	journal = {Test},
	author = {Winkler, R. L. and Muñoz, Javier and Cervera, José L. and Bernardo, José M. and Blattenberger, Gail and Kadane, Joseph B. and Lindley, Dennis V. and Murphy, Allan H. and Oliver, Robert M. and Ríos-Insua, David},
	month = jun,
	year = {1996},
	note = {158 citations (Crossref) [2023-10-17]},
	keywords = {evaluate-time-series},
	pages = {1--60},
}

@misc{max-planck-institut_fuer_biogeochemie_weather_2008,
	title = {Weather {Data}},
	url = {https://www.bgc-jena.mpg.de/wetter/},
	urldate = {2023-10-31},
	author = {{Max-Planck-Institut fuer Biogeochemie}},
	year = {2008},
}

@misc{cuturi_pems-sf_2011,
	title = {{PEMS}-{SF}},
	url = {https://archive.ics.uci.edu/dataset/204},
	doi = {10.24432/C52G70},
	urldate = {2023-10-30},
	publisher = {UCI Machine Learning Repository},
	author = {Cuturi, Marco},
	year = {2011},
	keywords = {dataset, traffic},
}

@article{hyvarinen_estimation_2005,
	title = {Estimation of {Non}-{Normalized} {Statistical} {Models} by {Score} {Matching}},
	volume = {6},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v6/hyvarinen05a.html},
	abstract = {One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simplifies to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete filter set for natural image data.},
	number = {24},
	urldate = {2023-11-14},
	journal = {Journal of Machine Learning Research},
	author = {Hyvärinen, Aapo},
	year = {2005},
	keywords = {⛔ No DOI found},
	pages = {695--709},
}

@article{gneiting_strictly_2007,
	title = {Strictly {Proper} {Scoring} {Rules}, {Prediction}, and {Estimation}},
	volume = {102},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214506000001437},
	doi = {10.1198/016214506000001437},
	abstract = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distributionF if he or she issues the probabilistic forecast F, rather than G ≠ F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the Savage representation. Examples of scoring rules for probabilistic forecasts in the form of predictive densities include the logarithmic, spherical, pseudospherical, and quadratic scores. The continuous ranked probability score applies to probabilistic forecasts that take the form of predictive cumulative distribution functions. It generalizes the absolute error and forms a special case of a new and very general type of score, the energy score. Like many other scoring rules, the energy score admits a kernel representation in terms of negative definite functions, with links to inequalities of Hoeffding type, in both univariate and multivariate settings. Proper scoring rules for quantile and interval forecasts are also discussed. We relate proper scoring rules to Bayes factors and to cross-validation, and propose a novel form of cross-validation known as random-fold cross-validation. A case study on probabilistic weather forecasts in the North American Pacific Northwest illustrates the importance of propriety. We note optimum score approaches to point and quantile estimation, and propose the intuitively appealing interval score as a utility function in interval estimation that addresses width as well as coverage.},
	number = {477},
	urldate = {2023-10-05},
	journal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann and Raftery, Adrian E},
	month = mar,
	year = {2007},
	note = {2669 citations (Crossref) [2023-10-17]
Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/016214506000001437},
	keywords = {CRPS, evaluate-time-series},
	pages = {359--378},
}

@misc{energy_online_caiso_2012,
	title = {{CAISO}: {Actual} {Load} {Data}},
	url = {http://www.energyonline.com/Data/GenericData.aspx?DataId=18&CAISO___Actual_Load},
	urldate = {2023-10-31},
	author = {{Energy Online}},
	year = {2012},
}

@article{lippi_short-term_2013,
	title = {Short-{Term} {Traffic} {Flow} {Forecasting}: {An} {Experimental} {Comparison} of {Time}-{Series} {Analysis} and {Supervised} {Learning}},
	volume = {14},
	issn = {1558-0016},
	shorttitle = {Short-{Term} {Traffic} {Flow} {Forecasting}},
	url = {https://ieeexplore.ieee.org/abstract/document/6482260},
	doi = {10.1109/TITS.2013.2247040},
	abstract = {The literature on short-term traffic flow forecasting has undergone great development recently. Many works, describing a wide variety of different approaches, which very often share similar features and ideas, have been published. However, publications presenting new prediction algorithms usually employ different settings, data sets, and performance measurements, making it difficult to infer a clear picture of the advantages and limitations of each model. The aim of this paper is twofold. First, we review existing approaches to short-term traffic flow forecasting methods under the common view of probabilistic graphical models, presenting an extensive experimental comparison, which proposes a common baseline for their performance analysis and provides the infrastructure to operate on a publicly available data set. Second, we present two new support vector regression models, which are specifically devised to benefit from typical traffic flow seasonality and are shown to represent an interesting compromise between prediction accuracy and computational efficiency. The SARIMA model coupled with a Kalman filter is the most accurate model; however, the proposed seasonal support vector regressor turns out to be highly competitive when performing forecasts during the most congested periods.},
	number = {2},
	urldate = {2023-11-08},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Lippi, Marco and Bertini, Matteo and Frasconi, Paolo},
	month = jun,
	year = {2013},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems},
	pages = {871--882},
}

@article{penfold_use_2013,
	series = {Quality {Improvement} in {Pediatric} {Health} {Care}},
	title = {Use of {Interrupted} {Time} {Series} {Analysis} in {Evaluating} {Health} {Care} {Quality} {Improvements}},
	volume = {13},
	issn = {1876-2859},
	url = {https://www.sciencedirect.com/science/article/pii/S1876285913002106},
	doi = {10.1016/j.acap.2013.08.002},
	abstract = {Interrupted time series (ITS) analysis is arguably the strongest quasi-experimental research design. ITS is particularly useful when a randomized trial is infeasible or unethical. The approach usually involves constructing a time series of population-level rates for a particular quality improvement focus (eg, rates of attention-deficit/hyperactivity disorder [ADHD] medication initiation) and testing statistically for a change in the outcome rate in the time periods before and time periods after implementation of a policy/program designed to change the outcome. In parallel, investigators often analyze rates of negative outcomes that might be (unintentionally) affected by the policy/program. We discuss why ITS is a useful tool for quality improvement. Strengths of ITS include the ability to control for secular trends in the data (unlike a 2-period before-and-after t test), ability to evaluate outcomes using population-level data, clear graphical presentation of results, ease of conducting stratified analyses, and ability to evaluate both intended and unintended consequences of interventions. Limitations of ITS include the need for a minimum of 8 time periods before and 8 after an intervention to evaluate changes statistically, difficulty in analyzing the independent impact of separate components of a program that are implemented close together in time, and existence of a suitable control population. Investigators must also be careful not to make individual-level inferences when population-level rates are used to evaluate interventions (though ITS can be used with individual-level data). A brief description of ITS is provided, including a fully implemented (but hypothetical) study of the impact of a program to reduce ADHD medication initiation in children younger than 5 years old and insured by Medicaid in Washington State. An example of the database needed to conduct an ITS is provided, as well as SAS code to implement a difference-in-differences model using preschool-age children in California as a comparison group.},
	number = {6, Supplement},
	urldate = {2023-11-08},
	journal = {Academic Pediatrics},
	author = {Penfold, Robert B. and Zhang, Fang},
	month = nov,
	year = {2013},
	note = {674 citations (Crossref) [2023-11-08]},
	keywords = {healthcare},
	pages = {S38--S44},
}

@misc{johnson_mimic-iii_2015,
	title = {{MIMIC}-{III} {Clinical} {Database}},
	url = {https://physionet.org/content/mimiciii/1.4/},
	doi = {10.13026/C2XW26},
	abstract = {MIMIC-III is a large, freely-available database comprising deidentified
health-related data associated with over forty thousand patients who stayed in
critical care units of the Beth Israel Deaconess Medical Center between 2001
and 2012. The database includes information such as demographics, vital sign
measurements made at the bedside ({\textasciitilde}1 data point per hour), laboratory test
results, procedures, medications, caregiver notes, imaging reports, and
mortality (including post-hospital discharge).

MIMIC supports a diverse range of analytic studies spanning epidemiology,
clinical decision-rule improvement, and electronic tool development. It is
notable for three factors: it is freely available to researchers worldwide; it
encompasses a diverse and very large population of ICU patients; and it
contains highly granular data, including vital signs, laboratory results, and
medications.},
	urldate = {2023-10-31},
	publisher = {PhysioNet},
	author = {Johnson, Alistair and Pollard, Tom and Mark, Roger},
	year = {2015},
}

@misc{nyc_taxi_and_limousine_commission_tlc_2015,
	title = {{TLC} {Trip} {Record} {Data} - {TLC}},
	url = {https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page},
	urldate = {2023-10-30},
	author = {{NYC Taxi and Limousine Commission}},
	year = {2015},
	keywords = {dataset, taxi},
}

@misc{sutskever_sequence_2014,
	title = {Sequence to {Sequence} {Learning} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/1409.3215},
	doi = {10.48550/arXiv.1409.3215},
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	month = dec,
	year = {2014},
	note = {arXiv:1409.3215 [cs]},
}

@inproceedings{sohl-dickstein_deep_2015,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	url = {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	language = {en},
	urldate = {2023-10-16},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	pages = {2256--2265},
}

@misc{oord_wavenet_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2023-10-09},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv:1609.03499 [cs]},
}

@inproceedings{papamakarios_masked_2017,
	title = {Masked {Autoregressive} {Flow} for {Density} {Estimation}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/6c1da886822c67822bcf3679d04369fa-Abstract.html},
	abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
	urldate = {2023-09-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
	year = {2017},
}

@misc{trindade_electricityloaddiagrams20112014_2015,
	title = {{ElectricityLoadDiagrams20112014}},
	url = {https://archive.ics.uci.edu/dataset/321},
	doi = {10.24432/C58C86},
	urldate = {2023-10-30},
	publisher = {UCI Machine Learning Repository},
	author = {Trindade, Artur},
	year = {2015},
	keywords = {dataset, electricity},
}

@inproceedings{rezende_variational_2015,
	title = {Variational {Inference} with {Normalizing} {Flows}},
	url = {https://proceedings.mlr.press/v37/rezende15.html},
	abstract = {The choice of the approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
	language = {en},
	urldate = {2023-10-23},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Rezende, Danilo and Mohamed, Shakir},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	keywords = {NF},
	pages = {1530--1538},
}

@misc{de_brebisson_artificial_2015,
	title = {Artificial {Neural} {Networks} {Applied} to {Taxi} {Destination} {Prediction}},
	url = {http://arxiv.org/abs/1508.00021},
	doi = {10.48550/arXiv.1508.00021},
	abstract = {We describe our first-place solution to the ECML/PKDD discovery challenge on taxi destination prediction. The task consisted in predicting the destination of a taxi based on the beginning of its trajectory, represented as a variable-length sequence of GPS points, and diverse associated meta-information, such as the departure time, the driver id and client information. Contrary to most published competitor approaches, we used an almost fully automated approach based on neural networks and we ranked first out of 381 teams. The architectures we tried use multi-layer perceptrons, bidirectional recurrent neural networks and models inspired from recently introduced memory networks. Our approach could easily be adapted to other applications in which the goal is to predict a fixed-length output from a variable-length sequence.},
	urldate = {2023-09-27},
	publisher = {arXiv},
	author = {de Brébisson, Alexandre and Simon, Étienne and Auvolat, Alex and Vincent, Pascal and Bengio, Yoshua},
	month = sep,
	year = {2015},
	note = {arXiv:1508.00021 [cs]},
}

@inproceedings{fraccaro_disentangled_2017,
	title = {A {Disentangled} {Recognition} and {Nonlinear} {Dynamics} {Model} for {Unsupervised} {Learning}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/7b7a53e239400a13bd6be6c91c4f6c4e-Abstract.html},
	abstract = {This paper takes a step towards temporal reasoning in a dynamically changing video, not in the pixel space that constitutes its frames, but in a latent space that describes the non-linear dynamics of the objects in its world. We introduce the Kalman variational auto-encoder, a framework for unsupervised learning of sequential data that disentangles two latent representations: an object's representation, coming from a recognition model, and a latent state describing its dynamics. As a result, the evolution of the world can be imagined and missing data imputed, both without the need to  generate high dimensional frames at each time step. The model is trained end-to-end on videos of a variety of simulated physical systems, and outperforms competing methods in generative and missing data imputation tasks.},
	urldate = {2023-09-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Fraccaro, Marco and Kamronn, Simon and Paquet, Ulrich and Winther, Ole},
	year = {2017},
	keywords = {KVAE},
}

@article{pavlyuk_short-term_2017,
	series = {{RelStat}-2016: {Proceedings} of the 16th {International} {Scientific} {Conference} {Reliability} and {Statistics} in {Transportation} and {Communication} {October} 19-22, 2016. {Transport} and {Telecommunication} {Institute}, {Riga}, {Latvia}},
	title = {Short-term {Traffic} {Forecasting} {Using} {Multivariate} {Autoregressive} {Models}},
	volume = {178},
	issn = {1877-7058},
	url = {https://www.sciencedirect.com/science/article/pii/S1877705817300620},
	doi = {10.1016/j.proeng.2017.01.062},
	abstract = {This research is devoted to a systematic review of multivariate models in the context of their application to short-term traffic flow forecasting. A set of discussed models includes autoregressive integrated moving average models (ARIMA and VARMA), error correction models (VECM and EC-VARMA), space-time ARMA (STARMA), and multivariate autoregressive space state models (MARSS). All these models are based on different assumptions about a structure of interrelationships in traffic data (in time, in space or between different traffic characteristics). We discussed base assumptions of these models (such as stationary of traffic flows and spatial independence of data) and their importance in the domain of transport flows. The discussion is supplemented with an empirical application of the models to forecasting of traffic speeds for a small road segment. Empirical conclusions and projected research directions are also presented.},
	urldate = {2023-11-08},
	journal = {Procedia Engineering},
	author = {Pavlyuk, Dmitry},
	month = jan,
	year = {2017},
	keywords = {traffic},
	pages = {57--66},
}

@misc{dinh_density_2017,
	title = {Density estimation using {Real} {NVP}},
	url = {http://arxiv.org/abs/1605.08803},
	doi = {10.48550/arXiv.1605.08803},
	abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
	month = feb,
	year = {2017},
	note = {arXiv:1605.08803 [cs, stat]},
}

@article{deb_review_2017,
	title = {A review on time series forecasting techniques for building energy consumption},
	volume = {74},
	issn = {1364-0321},
	url = {https://www.sciencedirect.com/science/article/pii/S1364032117303155},
	doi = {10.1016/j.rser.2017.02.085},
	abstract = {Energy consumption forecasting for buildings has immense value in energy efficiency and sustainability research. Accurate energy forecasting models have numerous implications in planning and energy optimization of buildings and campuses. For new buildings, where past recorded data is unavailable, computer simulation methods are used for energy analysis and forecasting future scenarios. However, for existing buildings with historically recorded time series energy data, statistical and machine learning techniques have proved to be more accurate and quick. This study presents a comprehensive review of the existing machine learning techniques for forecasting time series energy consumption. Although the emphasis is given to a single time series data analysis, the review is not just limited to it since energy data is often co-analyzed with other time series variables like outdoor weather and indoor environmental conditions. The nine most popular forecasting techniques that are based on the machine learning platform are analyzed. An in-depth review and analysis of the ‘hybrid model’, that combines two or more forecasting techniques is also presented. The various combinations of the hybrid model are found to be the most effective in time series energy forecasting for building.},
	urldate = {2023-11-08},
	journal = {Renewable and Sustainable Energy Reviews},
	author = {Deb, Chirag and Zhang, Fan and Yang, Junjing and Lee, Siew Eang and Shah, Kwok Wei},
	month = jul,
	year = {2017},
	keywords = {energy},
	pages = {902--924},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-09-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	keywords = {Transformer},
}

@inproceedings{bui_time_2018,
	address = {Singapore},
	series = {{IFMBE} {Proceedings}},
	title = {Time {Series} {Forecasting} for {Healthcare} {Diagnosis} and {Prognostics} with the {Focus} on {Cardiovascular} {Diseases}},
	isbn = {978-981-10-4361-1},
	doi = {10.1007/978-981-10-4361-1_138},
	abstract = {Time series forecasting has been a prosperous filed of science due to its popularity in real-world applications, yet being challenge in method developments. In medical applications, time series forecasting models have been successfully applied to predict the progress of the disease, estimate the mortality rate and assess the time dependent risk. However, the vast availability of many different techniques, in which each type excels in particular situations, makes the process of choosing an appropriate model more challenging. Therefore, the aim of this paper is to summarize and review different types of forecasting model that have been tremendously cultured for medical purposes using time series based forecasting methods. For each type of model, we will list the current related research papers, briefly describe the underlying theories, and discuss its advantages and disadvantages within different clinical situations. At the end, this paper also provides a robust and purpose-oriented classification of about 60 different forecasting models, therefore providing a comprehensive references for scientists and researchers to determine the suitable forecasting models for their case of study.},
	language = {en},
	booktitle = {6th {International} {Conference} on the {Development} of {Biomedical} {Engineering} in {Vietnam} ({BME6})},
	publisher = {Springer},
	author = {Bui, C. and Pham, N. and Vo, A. and Tran, A. and Nguyen, A. and Le, T.},
	editor = {Vo Van, Toi and Nguyen Le, Thanh An and Nguyen Duc, Thang},
	year = {2018},
	note = {11 citations (Crossref) [2023-11-08]},
	keywords = {healthcare},
	pages = {809--818},
}

@inproceedings{chen_neural_2018,
	title = {Neural {Ordinary} {Differential} {Equations}},
	volume = {31},
	url = {https://papers.nips.cc/paper_files/paper/2018/hash/69386f6bb1dfed68692a24c8686939b9-Abstract.html},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	urldate = {2023-10-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
	year = {2018},
	keywords = {⛔ No DOI found},
}

@misc{lai_modeling_2018,
	title = {Modeling {Long}- and {Short}-{Term} {Temporal} {Patterns} with {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1703.07015},
	doi = {10.48550/arXiv.1703.07015},
	abstract = {Multivariate time series forecasting is an important machine learning problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. Temporal data arise in these real-world applications often involves a mixture of long-term and short-term patterns, for which traditional approaches such as Autoregressive models and Gaussian Process may fail. In this paper, we proposed a novel deep learning framework, namely Long- and Short-term Time-series network (LSTNet), to address this open challenge. LSTNet uses the Convolution Neural Network (CNN) and the Recurrent Neural Network (RNN) to extract short-term local dependency patterns among variables and to discover long-term patterns for time series trends. Furthermore, we leverage traditional autoregressive model to tackle the scale insensitive problem of the neural network model. In our evaluation on real-world data with complex mixtures of repetitive patterns, LSTNet achieved significant performance improvements over that of several state-of-the-art baseline methods. All the data and experiment codes are available online.},
	urldate = {2023-10-18},
	publisher = {arXiv},
	author = {Lai, Guokun and Chang, Wei-Cheng and Yang, Yiming and Liu, Hanxiao},
	month = apr,
	year = {2018},
	note = {arXiv:1703.07015 [cs]},
}

@inproceedings{lai_modeling_2018-1,
	address = {New York, NY, USA},
	series = {{SIGIR} '18},
	title = {Modeling {Long}- and {Short}-{Term} {Temporal} {Patterns} with {Deep} {Neural} {Networks}},
	isbn = {978-1-4503-5657-2},
	url = {https://dl.acm.org/doi/10.1145/3209978.3210006},
	doi = {10.1145/3209978.3210006},
	abstract = {Multivariate time series forecasting is an important machine learning problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. Temporal data arise in these real-world applications often involves a mixture of long-term and short-term patterns, for which traditional approaches such as Autoregressive models and Gaussian Process may fail. In this paper, we proposed a novel deep learning framework, namely Long- and Short-term Time-series network (LSTNet), to address this open challenge. LSTNet uses the Convolution Neural Network (CNN) and the Recurrent Neural Network (RNN) to extract short-term local dependency patterns among variables and to discover long-term patterns for time series trends. Furthermore, we leverage traditional autoregressive model to tackle the scale insensitive problem of the neural network model. In our evaluation on real-world data with complex mixtures of repetitive patterns, LSTNet achieved significant performance improvements over that of several state-of-the-art baseline methods. All the data and experiment codes are available online.},
	urldate = {2023-10-27},
	booktitle = {The 41st {International} {ACM} {SIGIR} {Conference} on {Research} \& {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Lai, Guokun and Chang, Wei-Cheng and Yang, Yiming and Liu, Hanxiao},
	month = jun,
	year = {2018},
	keywords = {Exchange, Solar, dataset},
	pages = {95--104},
}

@inproceedings{kim_disentangling_2018,
	title = {Disentangling by {Factorising}},
	url = {https://proceedings.mlr.press/v80/kim18b.html},
	abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon beta-VAE by providing a better trade-off between disentanglement and reconstruction quality and being more robust to the number of training iterations. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
	language = {en},
	urldate = {2023-11-28},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kim, Hyunjik and Mnih, Andriy},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {2649--2658},
}

@article{che_recurrent_2018,
	title = {Recurrent {Neural} {Networks} for {Multivariate} {Time} {Series} with {Missing} {Values}},
	volume = {8},
	copyright = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-24271-9},
	doi = {10.1038/s41598-018-24271-9},
	abstract = {Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provide useful insights for better understanding and utilization of missing values in time series analysis.},
	language = {en},
	number = {1},
	urldate = {2023-11-08},
	journal = {Scientific Reports},
	author = {Che, Zhengping and Purushotham, Sanjay and Cho, Kyunghyun and Sontag, David and Liu, Yan},
	month = apr,
	year = {2018},
	note = {858 citations (Crossref) [2023-11-08]
Number: 1
Publisher: Nature Publishing Group},
	keywords = {healthcare},
	pages = {6085},
}

@misc{zhang_mixup_2018,
	title = {mixup: {Beyond} {Empirical} {Risk} {Minimization}},
	shorttitle = {mixup},
	url = {http://arxiv.org/abs/1710.09412},
	doi = {10.48550/arXiv.1710.09412},
	abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
	urldate = {2023-10-18},
	publisher = {arXiv},
	author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
	month = apr,
	year = {2018},
	note = {arXiv:1710.09412 [cs, stat]},
}

@article{saremi_neural_2019,
	title = {Neural {Empirical} {Bayes}},
	volume = {20},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v20/19-216.html},
	number = {181},
	urldate = {2023-11-28},
	journal = {Journal of Machine Learning Research},
	author = {Saremi, Saeed and Hyvärinen, Aapo},
	year = {2019},
	keywords = {⛔ No DOI found},
	pages = {1--23},
}

@inproceedings{gasthaus_probabilistic_2019,
	title = {Probabilistic {Forecasting} with {Spline} {Quantile} {Function} {RNNs}},
	url = {https://proceedings.mlr.press/v89/gasthaus19a.html},
	abstract = {In this paper, we propose a flexible method for probabilistic modeling with conditional quantile functions using monotonic regression splines. The shape of the spline is parameterized by a neural network whose parameters are learned by minimizing the continuous ranked probability score. Within this framework, we propose a method for probabilistic time series forecasting, which combines the modeling capacity of recurrent neural networks with the flexibility of a spline-based representation of the output distribution. Unlike methods based on parametric probability density functions and maximum likelihood estimation, the proposed method can flexibly adapt to different output distributions without manual intervention. We empirically demonstrate the effectiveness of the approach  on synthetic and real-world data sets.},
	language = {en},
	urldate = {2023-10-30},
	booktitle = {Proceedings of the {Twenty}-{Second} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Gasthaus, Jan and Benidis, Konstantinos and Wang, Yuyang and Rangapuram, Syama Sundar and Salinas, David and Flunkert, Valentin and Januschowski, Tim},
	month = apr,
	year = {2019},
	note = {ISSN: 2640-3498},
	keywords = {dataset, wiki},
	pages = {1901--1910},
}

@misc{grathwohl_ffjord_2018,
	title = {{FFJORD}: {Free}-form {Continuous} {Dynamics} for {Scalable} {Reversible} {Generative} {Models}},
	shorttitle = {{FFJORD}},
	url = {http://arxiv.org/abs/1810.01367},
	doi = {10.48550/arXiv.1810.01367},
	abstract = {A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
	month = oct,
	year = {2018},
	note = {arXiv:1810.01367 [cs, stat]},
}

@article{chou_forecasting_2018,
	title = {Forecasting energy consumption time series using machine learning techniques based on usage patterns of residential householders},
	volume = {165},
	issn = {0360-5442},
	url = {https://www.sciencedirect.com/science/article/pii/S0360544218319145},
	doi = {10.1016/j.energy.2018.09.144},
	abstract = {Energy consumption in buildings is increasing because of social development and urbanization. Forecasting the energy consumption in buildings is essential for improving energy efficiency and sustainable development, and thereby reducing energy costs and environmental impact. This investigation presents a comprehensive review of machine learning (ML) techniques for forecasting energy consumption time series using actual data. Real-time data were collected from a smart grid that was installed in an experimental building and used to evaluate the efficacy and effectiveness of statistical and ML techniques. Well-known artificial intelligence techniques were used to analyze energy consumption in single and ensemble scenarios. An in-depth review and analysis of the ‘hybrid model’ that combines forecasting and optimization techniques is presented. The comprehensive comparison demonstrates that the hybrid model is more accurate than the single and ensemble models. Both the accuracy of prediction and the suitability for use of these models are considered to support users in planning energy management.},
	urldate = {2023-11-08},
	journal = {Energy},
	author = {Chou, Jui-Sheng and Tran, Duc-Son},
	month = dec,
	year = {2018},
	keywords = {energy},
	pages = {709--726},
}

@article{salinas_high-dimensional_2019,
	title = {High-dimensional multivariate forecasting with low-rank {Gaussian} {Copula} {Processes}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/0b105cf1504c4e241fcc6d519ea962fb-Abstract.html?ref=https://githubhelp.com},
	language = {en},
	urldate = {2023-09-27},
	journal = {Advances in Neural Information Processing Systems},
	author = {Salinas, David and Bohlke-Schneider, Michael and Callot, Laurent and Medico, Roberto and Gasthaus, Jan},
	year = {2019},
	keywords = {GP-Copula, GP-scaling, Vec-LSTM-ind-scaling, Vec-LSTM-lowrank-Copula},
}

@inproceedings{song_generative_2019,
	title = {Generative {Modeling} by {Estimating} {Gradients} of the {Data} {Distribution}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html},
	abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples 
comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
	urldate = {2023-10-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Song, Yang and Ermon, Stefano},
	year = {2019},
	keywords = {diffusion},
}

@misc{salinas_high-dimensional_2019-1,
	title = {High-{Dimensional} {Multivariate} {Forecasting} with {Low}-{Rank} {Gaussian} {Copula} {Processes}},
	url = {http://arxiv.org/abs/1910.03002},
	abstract = {Predicting the dependencies between observations from multiple time series is critical for applications such as anomaly detection, financial risk management, causal analysis, or demand forecasting. However, the computational and numerical difficulties of estimating time-varying and high-dimensional covariance matrices often limits existing methods to handling at most a few hundred dimensions or requires making strong assumptions on the dependence between series. We propose to combine an RNN-based time series model with a Gaussian copula process output model with a low-rank covariance structure to reduce the computational complexity and handle non-Gaussian marginal distributions. This permits to drastically reduce the number of parameters and consequently allows the modeling of time-varying correlations of thousands of time series. We show on several real-world datasets that our method provides significant accuracy improvements over state-of-the-art baselines and perform an ablation study analyzing the contributions of the different components of our model.},
	urldate = {2023-10-30},
	publisher = {arXiv},
	author = {Salinas, David and Bohlke-Schneider, Michael and Callot, Laurent and Medico, Roberto and Gasthaus, Jan},
	month = oct,
	year = {2019},
	note = {arXiv:1910.03002 [cs, stat]},
}

@misc{alexandrov_gluonts_2019,
	title = {{GluonTS}: {Probabilistic} {Time} {Series} {Models} in {Python}},
	shorttitle = {{GluonTS}},
	url = {http://arxiv.org/abs/1906.05264},
	doi = {10.48550/arXiv.1906.05264},
	abstract = {We introduce Gluon Time Series (GluonTS, available at https://gluon-ts.mxnet.io), a library for deep-learning-based time series modeling. GluonTS simplifies the development of and experimentation with time series models for common tasks such as forecasting or anomaly detection. It provides all necessary components and tools that scientists need for quickly building new models, for efficiently running and analyzing experiments and for evaluating model accuracy.},
	urldate = {2023-10-30},
	publisher = {arXiv},
	author = {Alexandrov, Alexander and Benidis, Konstantinos and Bohlke-Schneider, Michael and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim and Maddix, Danielle C. and Rangapuram, Syama and Salinas, David and Schulz, Jasper and Stella, Lorenzo and Türkmen, Ali Caner and Wang, Yuyang},
	month = jun,
	year = {2019},
	note = {arXiv:1906.05264 [cs, stat]},
}

@article{jordan_evaluating_2019,
	title = {Evaluating {Probabilistic} {Forecasts} with {scoringRules}},
	volume = {90},
	copyright = {Copyright (c) 2019 Alexander Jordan, Fabian Krüger, Sebastian Lerch},
	issn = {1548-7660},
	url = {https://doi.org/10.18637/jss.v090.i12},
	doi = {10.18637/jss.v090.i12},
	abstract = {Probabilistic forecasts in the form of probability distributions over future events have become popular in several fields including meteorology, hydrology, economics, and demography. In typical applications, many alternative statistical models and data sources can be used to produce probabilistic forecasts. Hence, evaluating and selecting among competing methods is an important task. The scoringRules package for R provides functionality for comparative evaluation of probabilistic models based on proper scoring rules, covering a wide range of situations in applied work. This paper discusses implementation and usage details, presents case studies from meteorology and economics, and points to the relevant background literature.},
	language = {en},
	urldate = {2023-11-15},
	journal = {Journal of Statistical Software},
	author = {Jordan, Alexander and Krüger, Fabian and Lerch, Sebastian},
	month = aug,
	year = {2019},
	pages = {1--37},
}

@misc{de_brouwer_gru-ode-bayes_2019,
	title = {{GRU}-{ODE}-{Bayes}: {Continuous} modeling of sporadically-observed time series},
	shorttitle = {{GRU}-{ODE}-{Bayes}},
	url = {http://arxiv.org/abs/1905.12374},
	abstract = {Modeling real-world multidimensional time series can be particularly challenging when these are sporadically observed (i.e., sampling is irregular both in time and across dimensions)-such as in the case of clinical patient data. To address these challenges, we propose (1) a continuous-time version of the Gated Recurrent Unit, building upon the recent Neural Ordinary Differential Equations (Chen et al., 2018), and (2) a Bayesian update network that processes the sporadic observations. We bring these two ideas together in our GRU-ODE-Bayes method. We then demonstrate that the proposed method encodes a continuity prior for the latent process and that it can exactly represent the Fokker-Planck dynamics of complex processes driven by a multidimensional stochastic differential equation. Additionally, empirical evaluation shows that our method outperforms the state of the art on both synthetic data and real-world data with applications in healthcare and climate forecast. What is more, the continuity prior is shown to be well suited for low number of samples settings.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {De Brouwer, Edward and Simm, Jaak and Arany, Adam and Moreau, Yves},
	month = nov,
	year = {2019},
	note = {arXiv:1905.12374 [cs, stat]},
}

@misc{winkler_learning_2019,
	title = {Learning {Likelihoods} with {Conditional} {Normalizing} {Flows}},
	url = {http://arxiv.org/abs/1912.00042},
	doi = {10.48550/arXiv.1912.00042},
	abstract = {Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y{\textbar}x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.},
	urldate = {2023-10-05},
	publisher = {arXiv},
	author = {Winkler, Christina and Worrall, Daniel and Hoogeboom, Emiel and Welling, Max},
	month = nov,
	year = {2019},
	note = {arXiv:1912.00042 [cs, stat]},
}

@misc{li_learning_2019,
	title = {Learning {Energy}-{Based} {Models} in {High}-{Dimensional} {Spaces} with {Multi}-scale {Denoising} {Score} {Matching}},
	url = {http://arxiv.org/abs/1910.07762},
	doi = {10.48550/arXiv.1910.07762},
	abstract = {Energy-Based Models (EBMs) assign unnormalized log-probability to data samples. This functionality has a variety of applications, such as sample synthesis, data denoising, sample restoration, outlier detection, Bayesian reasoning, and many more. But training of EBMs using standard maximum likelihood is extremely slow because it requires sampling from the model distribution. Score matching potentially alleviates this problem. In particular, denoising score matching {\textbackslash}citep\{vincent2011connection\} has been successfully used to train EBMs. Using noisy data samples with one fixed noise level, these models learn fast and yield good results in data denoising {\textbackslash}citep\{saremi2019neural\}. However, demonstrations of such models in high quality sample synthesis of high dimensional data were lacking. Recently, {\textbackslash}citet\{song2019generative\} have shown that a generative model trained by denoising score matching accomplishes excellent sample synthesis, when trained with data samples corrupted with multiple levels of noise. Here we provide analysis and empirical evidence showing that training with multiple noise levels is necessary when the data dimension is high. Leveraging this insight, we propose a novel EBM trained with multi-scale denoising score matching. Our model exhibits data generation performance comparable to state-of-the-art techniques such as GANs, and sets a new baseline for EBMs. The proposed model also provides density information and performs well in an image inpainting task.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Li, Zengyi and Chen, Yubei and Sommer, Friedrich T.},
	month = dec,
	year = {2019},
	note = {arXiv:1910.07762 [cs, stat]},
}

@misc{nordpool_group_nordpool_2020,
	title = {{NordPool} {Market} {Data}},
	url = {https://www.nordpoolgroup.com/Market-data1/Power-system-data},
	abstract = {Nord Pool runs the leading power market in Europe, offering day-ahead and intraday markets to our customers.

Trade power with us in 16 countries and benefit from related services such as compliance, data and knowledge-sharing.},
	urldate = {2023-10-31},
	author = {{NordPool Group}},
	year = {2020},
}

@article{kaushik_ai_2020,
	title = {{AI} in {Healthcare}: {Time}-{Series} {Forecasting} {Using} {Statistical}, {Neural}, and {Ensemble} {Architectures}},
	volume = {3},
	issn = {2624-909X},
	shorttitle = {{AI} in {Healthcare}},
	url = {https://www.frontiersin.org/articles/10.3389/fdata.2020.00004},
	doi = {10.3389/fdata.2020.00004},
	abstract = {Both statistical and neural methods have been proposed in the literature to predict healthcare expenditures. However, less attention has been given to comparing predictions from both these methods as well as ensemble approaches in the healthcare domain. The primary objective of this paper was to evaluate different statistical, neural, and ensemble techniques in their ability to predict patients' weekly average expenditures on certain pain medications. Two statistical models, persistence (baseline) and autoregressive integrated moving average (ARIMA), a multilayer perceptron (MLP) model, a long short-term memory (LSTM) model, and an ensemble model combining predictions of the ARIMA, MLP, and LSTM models were calibrated to predict the expenditures on two different pain medications. In the MLP and LSTM models, we compared the influence of shuffling of training data and dropout of certain nodes in MLPs and nodes and recurrent connections in LSTMs in layers during training. Results revealed that the ensemble model outperformed the persistence, ARIMA, MLP, and LSTM models across both pain medications. In general, not shuffling the training data and adding the dropout helped the MLP models and shuffling the training data and not adding the dropout helped the LSTM models across both medications. We highlight the implications of using statistical, neural, and ensemble methods for time-series forecasting of outcomes in the healthcare domain.},
	urldate = {2023-11-08},
	journal = {Frontiers in Big Data},
	author = {Kaushik, Shruti and Choudhury, Abhinav and Sheron, Pankaj Kumar and Dasgupta, Nataraj and Natarajan, Sayee and Pickett, Larry A. and Dutt, Varun},
	year = {2020},
	note = {64 citations (Crossref) [2023-11-08]},
	keywords = {healthcare},
}

@article{chimmula_time_2020,
	title = {Time series forecasting of {COVID}-19 transmission in {Canada} using {LSTM} networks},
	volume = {135},
	issn = {0960-0779},
	url = {https://www.sciencedirect.com/science/article/pii/S0960077920302642},
	doi = {10.1016/j.chaos.2020.109864},
	abstract = {On March 11th 2020, World Health Organization (WHO) declared the 2019 novel corona virus as global pandemic. Corona virus, also known as COVID-19 was first originated in Wuhan, Hubei province in China around December 2019 and spread out all over the world within few weeks. Based on the public datasets provided by John Hopkins university and Canadian health authority, we have developed a forecasting model of COVID-19 outbreak in Canada using state-of-the-art Deep Learning (DL) models. In this novel research, we evaluated the key features to predict the trends and possible stopping time of the current COVID-19 outbreak in Canada and around the world. In this paper we presented the Long short-term memory (LSTM) networks, a deep learning approach to forecast the future COVID-19 cases. Based on the results of our Long short-term memory (LSTM) network, we predicted the possible ending point of this outbreak will be around June 2020. In addition to that, we compared transmission rates of Canada with Italy and USA. Here we also presented the 2, 4, 6, 8, 10, 12 and 14th day predictions for 2 successive days. Our forecasts in this paper is based on the available data until March 31, 2020. To the best of our knowledge, this of the few studies to use LSTM networks to forecast the infectious diseases.},
	urldate = {2023-11-07},
	journal = {Chaos, Solitons \& Fractals},
	author = {Chimmula, Vinay Kumar Reddy and Zhang, Lei},
	month = jun,
	year = {2020},
	note = {598 citations (Crossref) [2023-11-08]},
	keywords = {healthcare},
	pages = {109864},
}

@inproceedings{song_sliced_2020,
	title = {Sliced {Score} {Matching}: {A} {Scalable} {Approach} to {Density} and {Score} {Estimation}},
	shorttitle = {Sliced {Score} {Matching}},
	url = {https://proceedings.mlr.press/v115/song20a.html},
	abstract = {Score matching is a popular method for estimating unnormalized statistical models. However, it has been so far limited to simple, shallow models or low-dimensional data, due to the difficulty of computing the Hessian of log-density functions. We show this difficulty can be mitigated by projecting the scores onto random vectors before comparing them. This objective, called sliced score matching, only involves Hessian-vector products, which can be easily implemented using reverse-mode automatic differentiation. Therefore, sliced score matching is amenable to more complex models and higher dimensional data compared to score matching. Theoretically, we prove the consistency and asymptotic normality of sliced score matching estimators. Moreover, we demonstrate that sliced score matching can be used to learn deep score estimators for implicit distributions. In our experiments, we show sliced score matching can learn deep energy-based models effectively, and can produce accurate score estimates for applications such as variational inference with implicit distributions and training Wasserstein Auto-Encoders.},
	language = {en},
	urldate = {2023-11-14},
	booktitle = {Proceedings of {The} 35th {Uncertainty} in {Artificial} {Intelligence} {Conference}},
	publisher = {PMLR},
	author = {Song, Yang and Garg, Sahaj and Shi, Jiaxin and Ermon, Stefano},
	month = aug,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {574--584},
}

@misc{kong_diffwave_2020,
	title = {{DiffWave}: {A} {Versatile} {Diffusion} {Model} for {Audio} {Synthesis}},
	shorttitle = {{DiffWave}},
	url = {https://arxiv.org/abs/2009.09761v3},
	abstract = {In this work, we propose DiffWave, a versatile diffusion probabilistic model for conditional and unconditional waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in different waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.},
	language = {en},
	urldate = {2023-10-09},
	journal = {arXiv.org},
	author = {Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan},
	month = sep,
	year = {2020},
	keywords = {speech},
}

@misc{lim_temporal_2020,
	title = {Temporal {Fusion} {Transformers} for {Interpretable} {Multi}-horizon {Time} {Series} {Forecasting}},
	url = {http://arxiv.org/abs/1912.09363},
	abstract = {Multi-horizon forecasting problems often contain a complex mix of inputs -- including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed historically -- without any prior information on how they interact with the target. While several deep learning models have been proposed for multi-step prediction, they typically comprise black-box models which do not account for the full range of inputs present in common scenarios. In this paper, we introduce the Temporal Fusion Transformer (TFT) -- a novel attention-based architecture which combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, the TFT utilizes recurrent layers for local processing and interpretable self-attention layers for learning long-term dependencies. The TFT also uses specialized components for the judicious selection of relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of regimes. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and showcase three practical interpretability use-cases of TFT.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Lim, Bryan and Arik, Sercan O. and Loeff, Nicolas and Pfister, Tomas},
	month = sep,
	year = {2020},
	note = {arXiv:1912.09363 [cs, stat]},
}

@article{zeroual_deep_2020,
	title = {Deep learning methods for forecasting {COVID}-19 time-{Series} data: {A} {Comparative} study},
	volume = {140},
	issn = {0960-0779},
	shorttitle = {Deep learning methods for forecasting {COVID}-19 time-{Series} data},
	url = {https://www.sciencedirect.com/science/article/pii/S096007792030518X},
	doi = {10.1016/j.chaos.2020.110121},
	abstract = {The novel coronavirus (COVID-19) has significantly spread over the world and comes up with new challenges to the research community. Although governments imposing numerous containment and social distancing measures, the need for the healthcare systems has dramatically increased and the effective management of infected patients becomes a challenging problem for hospitals. Thus, accurate short-term forecasting of the number of new contaminated and recovered cases is crucial for optimizing the available resources and arresting or slowing down the progression of such diseases. Recently, deep learning models demonstrated important improvements when handling time-series data in different applications. This paper presents a comparative study of five deep learning methods to forecast the number of new cases and recovered cases. Specifically, simple Recurrent Neural Network (RNN), Long short-term memory (LSTM), Bidirectional LSTM (BiLSTM), Gated recurrent units (GRUs) and Variational AutoEncoder (VAE) algorithms have been applied for global forecasting of COVID-19 cases based on a small volume of data. This study is based on daily confirmed and recovered cases collected from six countries namely Italy, Spain, France, China, USA, and Australia. Results demonstrate the promising potential of the deep learning model in forecasting COVID-19 cases and highlight the superior performance of the VAE compared to the other algorithms.},
	urldate = {2023-11-07},
	journal = {Chaos, Solitons \& Fractals},
	author = {Zeroual, Abdelhafid and Harrou, Fouzi and Dairi, Abdelkader and Sun, Ying},
	month = nov,
	year = {2020},
	note = {285 citations (Crossref) [2023-11-08]},
	keywords = {healthcare},
	pages = {110121},
}

@article{salinas_deepar_2020,
	title = {{DeepAR}: {Probabilistic} forecasting with autoregressive recurrent networks},
	volume = {36},
	issn = {01692070},
	shorttitle = {{DeepAR}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207019301888},
	doi = {10.1016/j.ijforecast.2019.07.001},
	abstract = {Probabilistic forecasting, i.e., estimating a time series’ future probability distribution given its past, is a key enabler for optimizing business processes. In retail businesses, for example, probabilistic demand forecasts are crucial for having the right inventory available at the right time and in the right place. This paper proposes DeepAR, a methodology for producing accurate probabilistic forecasts, based on training an autoregressive recurrent neural network model on a large number of related time series. We demonstrate how the application of deep learning techniques to forecasting can overcome many of the challenges that are faced by widely-used classical approaches to the problem. By means of extensive empirical evaluations on several real-world forecasting datasets, we show that our methodology produces more accurate forecasts than other state-of-the-art methods, while requiring minimal manual work.},
	language = {en},
	number = {3},
	urldate = {2023-09-26},
	journal = {International Journal of Forecasting},
	author = {Salinas, David and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim},
	month = jul,
	year = {2020},
	note = {614 citations (Crossref) [2023-10-17]},
	pages = {1181--1191},
}

@misc{chen_residual_2020,
	title = {Residual {Flows} for {Invertible} {Generative} {Modeling}},
	url = {http://arxiv.org/abs/1906.02735},
	doi = {10.48550/arXiv.1906.02735},
	abstract = {Flow-based generative models parameterize probability distributions through an invertible transformation and can be trained by maximum likelihood. Invertible residual networks provide a flexible family of transformations where only Lipschitz conditions rather than strict architectural constraints are needed for enforcing invertibility. However, prior work trained invertible residual networks for density estimation by relying on biased log-density estimates whose bias increased with the network's expressiveness. We give a tractable unbiased estimate of the log density using a "Russian roulette" estimator, and reduce the memory required during training by using an alternative infinite series for the gradient. Furthermore, we improve invertible residual blocks by proposing the use of activation functions that avoid derivative saturation and generalizing the Lipschitz condition to induced mixed norms. The resulting approach, called Residual Flows, achieves state-of-the-art performance on density estimation amongst flow-based models, and outperforms networks that use coupling blocks at joint generative and discriminative modeling.},
	urldate = {2023-11-06},
	publisher = {arXiv},
	author = {Chen, Ricky T. Q. and Behrmann, Jens and Duvenaud, David and Jacobsen, Jörn-Henrik},
	month = jul,
	year = {2020},
	note = {arXiv:1906.02735 [cs, stat]},
}

@misc{pang_efficient_2020,
	title = {Efficient {Learning} of {Generative} {Models} via {Finite}-{Difference} {Score} {Matching}},
	url = {http://arxiv.org/abs/2007.03317},
	doi = {10.48550/arXiv.2007.03317},
	abstract = {Several machine learning applications involve the optimization of higher-order derivatives (e.g., gradients of gradients) during training, which can be expensive in respect to memory and computation even with automatic differentiation. As a typical example in generative modeling, score matching (SM) involves the optimization of the trace of a Hessian. To improve computing efficiency, we rewrite the SM objective and its variants in terms of directional derivatives, and present a generic strategy to efficiently approximate any-order directional derivative with finite difference (FD). Our approximation only involves function evaluations, which can be executed in parallel, and no gradient computations. Thus, it reduces the total computational cost while also improving numerical stability. We provide two instantiations by reformulating variants of SM objectives into the FD forms. Empirically, we demonstrate that our methods produce results comparable to the gradient-based counterparts while being much more computationally efficient.},
	urldate = {2023-11-14},
	publisher = {arXiv},
	author = {Pang, Tianyu and Xu, Kun and Li, Chongxuan and Song, Yang and Ermon, Stefano and Zhu, Jun},
	month = nov,
	year = {2020},
	note = {arXiv:2007.03317 [cs, stat]},
}

@misc{rasul_autoregressive_2021,
	title = {Autoregressive {Denoising} {Diffusion} {Models} for {Multivariate} {Probabilistic} {Time} {Series} {Forecasting}},
	url = {http://arxiv.org/abs/2101.12072},
	doi = {10.48550/arXiv.2101.12072},
	abstract = {In this work, we propose {\textbackslash}texttt\{TimeGrad\}, an autoregressive model for multivariate probabilistic time series forecasting which samples from the data distribution at each time step by estimating its gradient. To this end, we use diffusion probabilistic models, a class of latent variable models closely connected to score matching and energy-based methods. Our model learns gradients by optimizing a variational bound on the data likelihood and at inference time converts white noise into a sample of the distribution of interest through a Markov chain using Langevin sampling. We demonstrate experimentally that the proposed autoregressive denoising diffusion model is the new state-of-the-art multivariate probabilistic forecasting method on real-world data sets with thousands of correlated dimensions. We hope that this method is a useful tool for practitioners and lays the foundation for future research in this area.},
	urldate = {2023-10-24},
	publisher = {arXiv},
	author = {Rasul, Kashif and Seward, Calvin and Schuster, Ingmar and Vollgraf, Roland},
	month = feb,
	year = {2021},
	note = {arXiv:2101.12072 [cs]},
}

@misc{song_score-based_2021,
	title = {Score-{Based} {Generative} {Modeling} through {Stochastic} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2011.13456},
	doi = {10.48550/arXiv.2011.13456},
	abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field ({\textbackslash}aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.},
	urldate = {2023-10-19},
	publisher = {arXiv},
	author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
	month = feb,
	year = {2021},
	note = {arXiv:2011.13456 [cs, stat]},
	keywords = {diffusion},
}

@misc{nichol_improved_2021,
	title = {Improved {Denoising} {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2102.09672},
	doi = {10.48550/arXiv.2102.09672},
	abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Nichol, Alex and Dhariwal, Prafulla},
	month = feb,
	year = {2021},
	note = {arXiv:2102.09672 [cs, stat]},
	keywords = {improved\_diffusion},
}

@misc{papamakarios_normalizing_2021,
	title = {Normalizing {Flows} for {Probabilistic} {Modeling} and {Inference}},
	url = {http://arxiv.org/abs/1912.02762},
	abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
	urldate = {2023-10-05},
	publisher = {arXiv},
	author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
	month = apr,
	year = {2021},
	note = {arXiv:1912.02762 [cs, stat]},
}

@article{nguyen_temporal_2021,
	title = {Temporal {Latent} {Auto}-{Encoder}: {A} {Method} for {Probabilistic} {Multivariate} {Time} {Series} {Forecasting}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Temporal {Latent} {Auto}-{Encoder}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17101},
	doi = {10.1609/aaai.v35i10.17101},
	abstract = {Probabilistic forecasting of high dimensional multivariate time series is a notoriously challenging task, both in terms of computational burden and distribution modeling. Most previous work either makes simple distribution assumptions or abandons modeling cross-series correlations.  A promising line of work exploits scalable matrix factorization for latent-space forecasting, but is limited to linear embeddings, unable to model distributions, and not trainable end-to-end when using deep learning forecasting. We introduce a novel temporal latent auto-encoder method which enables nonlinear factorization of multivariate time series, learned end-to-end with a temporal deep learning latent space forecast model. By imposing a probabilistic latent space model, complex distributions of the input series are modeled via the decoder.  
 Extensive experiments demonstrate that our model achieves state-of-the-art performance on many popular multivariate datasets, with gains sometimes as high as 50\% for several standard metrics.},
	language = {en},
	number = {10},
	urldate = {2023-10-03},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Nguyen, Nam and Quanz, Brian},
	month = may,
	year = {2021},
	note = {Number: 10},
	pages = {9117--9125},
}

@article{zhou_informer_2021,
	title = {Informer: {Beyond} {Efficient} {Transformer} for {Long} {Sequence} {Time}-{Series} {Forecasting}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Informer},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17325},
	doi = {10.1609/aaai.v35i12.17325},
	abstract = {Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L log L) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.},
	language = {en},
	number = {12},
	urldate = {2023-10-30},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
	month = may,
	year = {2021},
	note = {Number: 12},
	keywords = {ETTh1, ETTm1, dataset},
	pages = {11106--11115},
}

@misc{papamakarios_normalizing_2021-1,
	title = {Normalizing {Flows} for {Probabilistic} {Modeling} and {Inference}},
	url = {http://arxiv.org/abs/1912.02762},
	doi = {10.48550/arXiv.1912.02762},
	abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
	urldate = {2023-11-06},
	publisher = {arXiv},
	author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
	month = apr,
	year = {2021},
	note = {arXiv:1912.02762 [cs, stat]},
}

@article{nguyen_temporal_2021-1,
	title = {Temporal {Latent} {Auto}-{Encoder}: {A} {Method} for {Probabilistic} {Multivariate} {Time} {Series} {Forecasting}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Temporal {Latent} {Auto}-{Encoder}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17101},
	doi = {10.1609/aaai.v35i10.17101},
	abstract = {Probabilistic forecasting of high dimensional multivariate time series is a notoriously challenging task, both in terms of computational burden and distribution modeling. Most previous work either makes simple distribution assumptions or abandons modeling cross-series correlations.  A promising line of work exploits scalable matrix factorization for latent-space forecasting, but is limited to linear embeddings, unable to model distributions, and not trainable end-to-end when using deep learning forecasting. We introduce a novel temporal latent auto-encoder method which enables nonlinear factorization of multivariate time series, learned end-to-end with a temporal deep learning latent space forecast model. By imposing a probabilistic latent space model, complex distributions of the input series are modeled via the decoder.  
 Extensive experiments demonstrate that our model achieves state-of-the-art performance on many popular multivariate datasets, with gains sometimes as high as 50\% for several standard metrics.},
	language = {en},
	number = {10},
	urldate = {2023-09-27},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Nguyen, Nam and Quanz, Brian},
	month = may,
	year = {2021},
	note = {Number: 10},
	pages = {9117--9125},
}

@misc{yan_scoregrad_2021,
	title = {{ScoreGrad}: {Multivariate} {Probabilistic} {Time} {Series} {Forecasting} with {Continuous} {Energy}-based {Generative} {Models}},
	shorttitle = {{ScoreGrad}},
	url = {http://arxiv.org/abs/2106.10121},
	doi = {10.48550/arXiv.2106.10121},
	abstract = {Multivariate time series prediction has attracted a lot of attention because of its wide applications such as intelligence transportation, AIOps. Generative models have achieved impressive results in time series modeling because they can model data distribution and take noise into consideration. However, many existing works can not be widely used because of the constraints of functional form of generative models or the sensitivity to hyperparameters. In this paper, we propose ScoreGrad, a multivariate probabilistic time series forecasting framework based on continuous energy-based generative models. ScoreGrad is composed of time series feature extraction module and conditional stochastic differential equation based score matching module. The prediction can be achieved by iteratively solving reverse-time SDE. To the best of our knowledge, ScoreGrad is the first continuous energy based generative model used for time series forecasting. Furthermore, ScoreGrad achieves state-of-the-art results on six real-world datasets. The impact of hyperparameters and sampler types on the performance are also explored. Code is available at https://github.com/yantijin/ScoreGradPred.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Yan, Tijin and Zhang, Hongwei and Zhou, Tong and Zhan, Yufeng and Xia, Yuanqing},
	month = jun,
	year = {2021},
	note = {arXiv:2106.10121 [cs, stat]},
	keywords = {Diffusion, ScoreGrad},
}

@inproceedings{wang_bicmts_2021,
	address = {New York, NY, USA},
	series = {{CIKM} '21},
	title = {{BiCMTS}: {Bidirectional} {Coupled} {Multivariate} {Learning} of {Irregular} {Time} {Series} with {Missing} {Values}},
	isbn = {978-1-4503-8446-9},
	shorttitle = {{BiCMTS}},
	url = {https://dl.acm.org/doi/10.1145/3459637.3482064},
	doi = {10.1145/3459637.3482064},
	abstract = {Multivariate time series (MTS) such as multiple medical measures in intensive care units (ICU) are irregularly acquired and hold missing values. Conducting learning tasks on such irregular MTS with missing values, e.g., predicting the mortality of ICU patients, poses significant challenge to existing MTS forecasting models and recurrent neural networks (RNNs), which capture the temporal dependencies within a time series. This work proposes a bidirectional coupled MTS learning (BiCMTS) method to represent both forward and backward value couplings within a time series by RNNs and between MTS by self-attention networks; the learned bidirectional intra- and inter-time series coupling representations are fused to estimate missing values. We test BiCMTS on both data imputation and mortality prediction for ICU patients, showing a great potential of leveraging the deep and hidden relations captured in RNNs by the BiCMTS-learned intra- and inter-time series value couplings in MTS.},
	urldate = {2023-10-04},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Qinfen and Ren, Siyuan and Xia, Yong and Cao, Longbing},
	month = oct,
	year = {2021},
	note = {3 citations (Crossref) [2023-10-17]},
	pages = {3493--3497},
}

@article{kobyzev_normalizing_2021,
	title = {Normalizing {Flows}: {An} {Introduction} and {Review} of {Current} {Methods}},
	volume = {43},
	issn = {1939-3539},
	shorttitle = {Normalizing {Flows}},
	url = {https://ieeexplore.ieee.org/document/9089305},
	doi = {10.1109/TPAMI.2020.2992934},
	abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
	number = {11},
	urldate = {2023-10-04},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kobyzev, Ivan and Prince, Simon J.D. and Brubaker, Marcus A.},
	month = nov,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {NF},
	pages = {3964--3979},
}

@inproceedings{tashiro_csdi_2021,
	title = {{CSDI}: {Conditional} {Score}-based {Diffusion} {Models} for {Probabilistic} {Time} {Series} {Imputation}},
	volume = {34},
	shorttitle = {{CSDI}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/cfe8504bda37b575c70ee1a8276f3486-Abstract.html},
	abstract = {The imputation of missing values in time series has many applications in healthcare and finance. While autoregressive models are natural candidates for time series imputation, score-based diffusion models have recently outperformed existing counterparts including autoregressive models in many tasks such as image generation and audio synthesis, and would be promising for time series imputation. In this paper, we propose Conditional Score-based Diffusion model (CSDI), a novel time series imputation method that utilizes score-based diffusion models conditioned on observed data. Unlike existing score-based approaches, the conditional diffusion model is explicitly trained for imputation and can exploit correlations between observed values. On healthcare and environmental data, CSDI improves by 40-65\% over existing probabilistic imputation methods on popular performance metrics. In addition, deterministic imputation by CSDI reduces the error by 5-20\% compared to the state-of-the-art deterministic imputation methods. Furthermore, CSDI can also be applied to time series interpolation and probabilistic forecasting, and is competitive with existing baselines. The code is available at https://github.com/ermongroup/CSDI.},
	urldate = {2023-09-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tashiro, Yusuke and Song, Jiaming and Song, Yang and Ermon, Stefano},
	month = nov,
	year = {2021},
	keywords = {CSDI, Diffusion},
	pages = {24804--24816},
}

@misc{ho_cascaded_2021,
	title = {Cascaded {Diffusion} {Models} for {High} {Fidelity} {Image} {Generation}},
	url = {http://arxiv.org/abs/2106.15282},
	doi = {10.48550/arXiv.2106.15282},
	abstract = {We show that cascaded diffusion models are capable of generating high fidelity images on the class-conditional ImageNet generation benchmark, without any assistance from auxiliary image classifiers to boost sample quality. A cascaded diffusion model comprises a pipeline of multiple diffusion models that generate images of increasing resolution, beginning with a standard diffusion model at the lowest resolution, followed by one or more super-resolution diffusion models that successively upsample the image and add higher resolution details. We find that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Our experiments show that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping us to train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at 128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and classification accuracy scores of 63.02\% (top-1) and 84.06\% (top-5) at 256x256, outperforming VQ-VAE-2.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Ho, Jonathan and Saharia, Chitwan and Chan, William and Fleet, David J. and Norouzi, Mohammad and Salimans, Tim},
	month = dec,
	year = {2021},
	note = {arXiv:2106.15282 [cs]},
	keywords = {image},
}

@article{dumas_deep_2022,
	title = {A deep generative model for probabilistic energy forecasting in power systems: normalizing flows},
	volume = {305},
	issn = {0306-2619},
	shorttitle = {A deep generative model for probabilistic energy forecasting in power systems},
	url = {https://www.sciencedirect.com/science/article/pii/S0306261921011909},
	doi = {10.1016/j.apenergy.2021.117871},
	abstract = {Greater direct electrification of end-use sectors with a higher share of renewables is one of the pillars to power a carbon-neutral society by 2050. However, in contrast to conventional power plants, renewable energy is subject to uncertainty raising challenges for their interaction with power systems. Scenario-based probabilistic forecasting models have become a vital tool to equip decision-makers. This paper presents to the power systems forecasting practitioners a recent deep learning technique, the normalizing flows, to produce accurate scenario-based probabilistic forecasts that are crucial to face the new challenges in power systems applications. The strength of this technique is to directly learn the stochastic multivariate distribution of the underlying process by maximizing the likelihood. Through comprehensive empirical evaluations using the open data of the Global Energy Forecasting Competition 2014, we demonstrate that this methodology is competitive with other state-of-the-art deep learning generative models: generative adversarial networks and variational autoencoders. The models producing weather-based wind, solar power, and load scenarios are properly compared in terms of forecast value by considering the case study of an energy retailer and quality using several complementary metrics. The numerical experiments are simple and easily reproducible. Thus, we hope it will encourage other forecasting practitioners to test and use normalizing flows in power system applications such as bidding on electricity markets, scheduling power systems with high renewable energy sources penetration, energy management of virtual power plan or microgrids, and unit commitment.},
	urldate = {2023-10-05},
	journal = {Applied Energy},
	author = {Dumas, Jonathan and Wehenkel, Antoine and Lanaspeze, Damien and Cornélusse, Bertrand and Sutera, Antonio},
	month = jan,
	year = {2022},
	note = {25 citations (Crossref) [2023-10-12]},
	keywords = {profile},
	pages = {117871},
}

@misc{wu_autoformer_2022,
	title = {Autoformer: {Decomposition} {Transformers} with {Auto}-{Correlation} for {Long}-{Term} {Series} {Forecasting}},
	shorttitle = {Autoformer},
	url = {http://arxiv.org/abs/2106.13008},
	doi = {10.48550/arXiv.2106.13008},
	abstract = {Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Going beyond Transformers, we design Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efficiency and accuracy. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38\% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease. Code is available at this repository: {\textbackslash}url\{https://github.com/thuml/Autoformer\}.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
	month = jan,
	year = {2022},
	note = {arXiv:2106.13008 [cs]},
	keywords = {electricity, ett\_small, exchange\_rate, illness, traffic, weather},
}

@misc{jamgochian_conditional_2022,
	title = {Conditional {Approximate} {Normalizing} {Flows} for {Joint} {Multi}-{Step} {Probabilistic} {Forecasting} with {Application} to {Electricity} {Demand}},
	url = {http://arxiv.org/abs/2201.02753},
	abstract = {Some real-world decision-making problems require making probabilistic forecasts over multiple steps at once. However, methods for probabilistic forecasting may fail to capture correlations in the underlying time-series that exist over long time horizons as errors accumulate. One such application is with resource scheduling under uncertainty in a grid environment, which requires forecasting electricity demand that is inherently noisy, but often cyclic. In this paper, we introduce the conditional approximate normalizing flow (CANF) to make probabilistic multi-step time-series forecasts when correlations are present over long time horizons. We first demonstrate our method's efficacy on estimating the density of a toy distribution, finding that CANF improves the KL divergence by one-third compared to that of a Gaussian mixture model while still being amenable to explicit conditioning. We then use a publicly available household electricity consumption dataset to showcase the effectiveness of CANF on joint probabilistic multi-step forecasting. Empirical results show that conditional approximate normalizing flows outperform other methods in terms of multi-step forecast accuracy and lead to up to 10x better scheduling decisions. Our implementation is available at https://github.com/sisl/JointDemandForecasting.},
	urldate = {2023-10-04},
	publisher = {arXiv},
	author = {Jamgochian, Arec and Wu, Di and Menda, Kunal and Jung, Soyeon and Kochenderfer, Mykel J.},
	month = jan,
	year = {2022},
	note = {arXiv:2201.02753 [cs]},
}

@misc{jamgochian_conditional_2022-1,
	title = {Conditional {Approximate} {Normalizing} {Flows} for {Joint} {Multi}-{Step} {Probabilistic} {Forecasting} with {Application} to {Electricity} {Demand}},
	url = {http://arxiv.org/abs/2201.02753},
	doi = {10.48550/arXiv.2201.02753},
	abstract = {Some real-world decision-making problems require making probabilistic forecasts over multiple steps at once. However, methods for probabilistic forecasting may fail to capture correlations in the underlying time-series that exist over long time horizons as errors accumulate. One such application is with resource scheduling under uncertainty in a grid environment, which requires forecasting electricity demand that is inherently noisy, but often cyclic. In this paper, we introduce the conditional approximate normalizing flow (CANF) to make probabilistic multi-step time-series forecasts when correlations are present over long time horizons. We first demonstrate our method's efficacy on estimating the density of a toy distribution, finding that CANF improves the KL divergence by one-third compared to that of a Gaussian mixture model while still being amenable to explicit conditioning. We then use a publicly available household electricity consumption dataset to showcase the effectiveness of CANF on joint probabilistic multi-step forecasting. Empirical results show that conditional approximate normalizing flows outperform other methods in terms of multi-step forecast accuracy and lead to up to 10x better scheduling decisions. Our implementation is available at https://github.com/sisl/JointDemandForecasting.},
	urldate = {2023-10-05},
	publisher = {arXiv},
	author = {Jamgochian, Arec and Wu, Di and Menda, Kunal and Jung, Soyeon and Kochenderfer, Mykel J.},
	month = jan,
	year = {2022},
	note = {arXiv:2201.02753 [cs]},
}

@misc{benny_dynamic_2022,
	title = {Dynamic {Dual}-{Output} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2203.04304},
	doi = {10.48550/arXiv.2203.04304},
	abstract = {Iterative denoising-based generation, also known as denoising diffusion models, has recently been shown to be comparable in quality to other classes of generative models, and even surpass them. Including, in particular, Generative Adversarial Networks, which are currently the state of the art in many sub-tasks of image generation. However, a major drawback of this method is that it requires hundreds of iterations to produce a competitive result. Recent works have proposed solutions that allow for faster generation with fewer iterations, but the image quality gradually deteriorates with increasingly fewer iterations being applied during generation. In this paper, we reveal some of the causes that affect the generation quality of diffusion models, especially when sampling with few iterations, and come up with a simple, yet effective, solution to mitigate them. We consider two opposite equations for the iterative denoising, the first predicts the applied noise, and the second predicts the image directly. Our solution takes the two options and learns to dynamically alternate between them through the denoising process. Our proposed solution is general and can be applied to any existing diffusion model. As we show, when applied to various SOTA architectures, our solution immediately improves their generation quality, with negligible added complexity and parameters. We experiment on multiple datasets and configurations and run an extensive ablation study to support these findings.},
	urldate = {2023-10-17},
	publisher = {arXiv},
	author = {Benny, Yaniv and Wolf, Lior},
	month = mar,
	year = {2022},
	note = {arXiv:2203.04304 [cs, eess]},
}

@misc{fan_depts_2022,
	title = {{DEPTS}: {Deep} {Expansion} {Learning} for {Periodic} {Time} {Series} {Forecasting}},
	shorttitle = {{DEPTS}},
	url = {http://arxiv.org/abs/2203.07681},
	doi = {10.48550/arXiv.2203.07681},
	abstract = {Periodic time series (PTS) forecasting plays a crucial role in a variety of industries to foster critical tasks, such as early warning, pre-planning, resource scheduling, etc. However, the complicated dependencies of the PTS signal on its inherent periodicity as well as the sophisticated composition of various periods hinder the performance of PTS forecasting. In this paper, we introduce a deep expansion learning framework, DEPTS, for PTS forecasting. DEPTS starts with a decoupled formulation by introducing the periodic state as a hidden variable, which stimulates us to make two dedicated modules to tackle the aforementioned two challenges. First, we develop an expansion module on top of residual learning to perform a layer-by-layer expansion of those complicated dependencies. Second, we introduce a periodicity module with a parameterized periodic function that holds sufficient capacity to capture diversified periods. Moreover, our two customized modules also have certain interpretable capabilities, such as attributing the forecasts to either local momenta or global periodicity and characterizing certain core periodic properties, e.g., amplitudes and frequencies. Extensive experiments on both synthetic data and real-world data demonstrate the effectiveness of DEPTS on handling PTS. In most cases, DEPTS achieves significant improvements over the best baseline. Specifically, the error reduction can even reach up to 20\% for a few cases. Finally, all codes are publicly available.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Fan, Wei and Zheng, Shun and Yi, Xiaohan and Cao, Wei and Fu, Yanjie and Bian, Jiang and Liu, Tie-Yan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.07681 [cs, stat]},
	keywords = {caiso, norpool},
}

@misc{rombach_high-resolution_2022,
	title = {High-{Resolution} {Image} {Synthesis} with {Latent} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2112.10752},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
	urldate = {2023-10-24},
	publisher = {arXiv},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	month = apr,
	year = {2022},
	note = {arXiv:2112.10752 [cs]},
	keywords = {image},
}

@misc{li_diffusion-lm_2022,
	title = {Diffusion-{LM} {Improves} {Controllable} {Text} {Generation}},
	url = {http://arxiv.org/abs/2205.14217},
	doi = {10.48550/arXiv.2205.14217},
	abstract = {Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Li, Xiang Lisa and Thickstun, John and Gulrajani, Ishaan and Liang, Percy and Hashimoto, Tatsunori B.},
	month = may,
	year = {2022},
	note = {arXiv:2205.14217 [cs]},
	keywords = {text},
}

@misc{ho_video_2022,
	title = {Video {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2204.03458},
	doi = {10.48550/arXiv.2204.03458},
	abstract = {Generating temporally coherent high fidelity video is an important milestone in generative modeling research. We make progress towards this milestone by proposing a diffusion model for video generation that shows very promising initial results. Our model is a natural extension of the standard image diffusion architecture, and it enables jointly training from image and video data, which we find to reduce the variance of minibatch gradients and speed up optimization. To generate long and higher resolution videos we introduce a new conditional sampling technique for spatial and temporal video extension that performs better than previously proposed methods. We present the first results on a large text-conditioned video generation task, as well as state-of-the-art results on established benchmarks for video prediction and unconditional video generation. Supplementary material is available at https://video-diffusion.github.io/},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey and Chan, William and Norouzi, Mohammad and Fleet, David J.},
	month = jun,
	year = {2022},
	note = {arXiv:2204.03458 [cs]},
	keywords = {video},
}

@book{foster_generative_2022,
	title = {Generative {Deep} {Learning}},
	isbn = {978-1-09-813414-3},
	abstract = {Generative AI is the hottest topic in tech. This practical book teaches machine learning engineers and data scientists how to use TensorFlow and Keras to create impressive generative deep learning models from scratch, including variational autoencoders (VAEs), generative adversarial networks (GANs), Transformers, normalizing flows, energy-based models, and denoising diffusion models.The book starts with the basics of deep learning and progresses to cutting-edge architectures. Through tips and tricks, you'll understand how to make your models learn more efficiently and become more creative.Discover how VAEs can change facial expressions in photosTrain GANs to generate images based on your own datasetBuild diffusion models to produce new varieties of flowersTrain your own GPT for text generationLearn how large language models like ChatGPT are trainedExplore state-of-the-art architectures such as StyleGAN2 and ViT-VQGANCompose polyphonic music using Transformers and MuseGANUnderstand how generative world models can solve reinforcement learning tasksDive into multimodal models such as DALL.E 2, Imagen, and Stable DiffusionThis book also explores the future of generative AI and how individuals and companies can proactively begin to leverage this remarkable new technology to create competitive advantage.},
	language = {en},
	publisher = {"O'Reilly Media, Inc."},
	author = {Foster, David},
	month = jun,
	year = {2022},
	note = {Google-Books-ID: BEq8EAAAQBAJ},
}

@inproceedings{goel_its_2022,
	title = {It’s {Raw}! {Audio} {Generation} with {State}-{Space} {Models}},
	url = {https://proceedings.mlr.press/v162/goel22a.html},
	abstract = {Developing architectures suitable for modeling raw audio is a challenging problem due to the high sampling rates of audio waveforms. Standard sequence modeling approaches like RNNs and CNNs have previously been tailored to fit the demands of audio, but the resultant architectures make undesirable computational tradeoffs and struggle to model waveforms effectively. We propose SaShiMi, a new multi-scale architecture for waveform modeling built around the recently introduced S4 model for long sequence modeling. We identify that S4 can be unstable during autoregressive generation, and provide a simple improvement to its parameterization by drawing connections to Hurwitz matrices. SaShiMi yields state-of-the-art performance for unconditional waveform generation in the autoregressive setting. Additionally, SaShiMi improves non-autoregressive generation performance when used as the backbone architecture for a diffusion model. Compared to prior architectures in the autoregressive generation setting, SaShiMi generates piano and speech waveforms which humans find more musical and coherent respectively, e.g. 2\{{\textbackslash}texttimes\} better mean opinion scores than WaveNet on an unconditional speech generation task. On a music generation task, SaShiMi outperforms WaveNet on density estimation and speed at both training and inference even when using 3\{{\textbackslash}texttimes\} fewer parameters},
	language = {en},
	urldate = {2023-10-03},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Goel, Karan and Gu, Albert and Donahue, Chris and Re, Christopher},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	keywords = {SaShiMi},
	pages = {7616--7633},
}

@misc{zhou_fedformer_2022,
	title = {{FEDformer}: {Frequency} {Enhanced} {Decomposed} {Transformer} for {Long}-term {Series} {Forecasting}},
	shorttitle = {{FEDformer}},
	url = {http://arxiv.org/abs/2201.12740},
	doi = {10.48550/arXiv.2201.12740},
	abstract = {Although Transformer-based methods have significantly improved state-of-the-art results for long-term series forecasting, they are not only computationally expensive but more importantly, are unable to capture the global view of time series (e.g. overall trend). To address these problems, we propose to combine Transformer with the seasonal-trend decomposition method, in which the decomposition method captures the global profile of time series while Transformers capture more detailed structures. To further enhance the performance of Transformer for long-term prediction, we exploit the fact that most time series tend to have a sparse representation in well-known basis such as Fourier transform, and develop a frequency enhanced Transformer. Besides being more effective, the proposed method, termed as Frequency Enhanced Decomposed Transformer (\{{\textbackslash}bf FEDformer\}), is more efficient than standard Transformer with a linear complexity to the sequence length. Our empirical studies with six benchmark datasets show that compared with state-of-the-art methods, FEDformer can reduce prediction error by \$14.8{\textbackslash}\%\$ and \$22.6{\textbackslash}\%\$ for multivariate and univariate time series, respectively. Code is publicly available at https://github.com/MAZiqing/FEDformer.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Wang, Xue and Sun, Liang and Jin, Rong},
	month = jun,
	year = {2022},
	note = {arXiv:2201.12740 [cs, stat]},
}

@article{he_catn_2022,
	title = {{CATN}: {Cross} {Attentive} {Tree}-{Aware} {Network} for {Multivariate} {Time} {Series} {Forecasting}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{CATN}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20320},
	doi = {10.1609/aaai.v36i4.20320},
	abstract = {Modeling complex hierarchical and grouped feature interaction in the multivariate time series data is indispensable to comprehend the data dynamics and predicting the future condition. The implicit feature interaction and high-dimensional data make multivariate forecasting very challenging. Many existing works did not put more emphasis on exploring explicit correlation among multiple time series data, and complicated models are designed to capture long- and short-range pattern with the aid of attention mechanism. In this work, we think that pre-defined graph or general learning method is difficult due to their irregular structure. Hence, we present CATN, an end-to-end model of Cross Attentive Tree-aware Network to jointly capture the inter-series correlation and intra-series temporal pattern. We first construct a tree structure to learn hierarchical and grouped correlation and design an embedding approach that can pass dynamic message to generalize implicit but interpretable cross features among multiple time series. Next in temporal aspect, we propose a multi-level dependency learning mechanism including global\&local learning and cross attention mechanism, which can combine long-range dependencies, short-range dependencies as well as cross dependencies at different time steps. The extensive experiments on different datasets from real world show the effectiveness and robustness of the method we proposed when compared with existing state-of-the-art methods.},
	language = {en},
	number = {4},
	urldate = {2023-10-04},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {He, Hui and Zhang, Qi and Bai, Simeng and Yi, Kun and Niu, Zhendong},
	month = jun,
	year = {2022},
	note = {2 citations (Crossref) [2023-10-17]
Number: 4},
	pages = {4030--4038},
}

@misc{gu_efficiently_2022,
	title = {Efficiently {Modeling} {Long} {Sequences} with {Structured} {State} {Spaces}},
	url = {http://arxiv.org/abs/2111.00396},
	doi = {10.48550/arXiv.2111.00396},
	abstract = {A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of \$10000\$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) {\textbackslash}( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) {\textbackslash}), and showed that for appropriate choices of the state matrix {\textbackslash}( A {\textbackslash}), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning {\textbackslash}( A {\textbackslash}) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91{\textbackslash}\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation \$60{\textbackslash}times\$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Gu, Albert and Goel, Karan and Ré, Christopher},
	month = aug,
	year = {2022},
	note = {arXiv:2111.00396 [cs]},
}

@misc{luo_understanding_2022,
	title = {Understanding {Diffusion} {Models}: {A} {Unified} {Perspective}},
	shorttitle = {Understanding {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2208.11970},
	abstract = {Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.},
	urldate = {2023-10-17},
	publisher = {arXiv},
	author = {Luo, Calvin},
	month = aug,
	year = {2022},
	note = {arXiv:2208.11970 [cs]},
}

@misc{luo_understanding_2022-1,
	title = {Understanding {Diffusion} {Models}: {A} {Unified} {Perspective}},
	shorttitle = {Understanding {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2208.11970},
	doi = {10.48550/arXiv.2208.11970},
	abstract = {Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Luo, Calvin},
	month = aug,
	year = {2022},
	note = {arXiv:2208.11970 [cs]},
	keywords = {explain},
}

@misc{lugmayr_repaint_2022,
	title = {{RePaint}: {Inpainting} using {Denoising} {Diffusion} {Probabilistic} {Models}},
	shorttitle = {{RePaint}},
	url = {http://arxiv.org/abs/2201.09865},
	doi = {10.48550/arXiv.2201.09865},
	abstract = {Free-form inpainting is the task of adding new content to an image in the regions specified by an arbitrary binary mask. Most existing approaches train for a certain distribution of masks, which limits their generalization capabilities to unseen mask types. Furthermore, training with pixel-wise and perceptual losses often leads to simple textural extensions towards the missing areas instead of semantically meaningful generation. In this work, we propose RePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting approach that is applicable to even extreme masks. We employ a pretrained unconditional DDPM as the generative prior. To condition the generation process, we only alter the reverse diffusion iterations by sampling the unmasked regions using the given image information. Since this technique does not modify or condition the original DDPM network itself, the model produces high-quality and diverse output images for any inpainting form. We validate our method for both faces and general-purpose image inpainting using standard and extreme masks. RePaint outperforms state-of-the-art Autoregressive, and GAN approaches for at least five out of six mask distributions. Github Repository: git.io/RePaint},
	urldate = {2023-10-16},
	publisher = {arXiv},
	author = {Lugmayr, Andreas and Danelljan, Martin and Romero, Andres and Yu, Fisher and Timofte, Radu and Van Gool, Luc},
	month = aug,
	year = {2022},
	note = {arXiv:2201.09865 [cs]},
}

@misc{poole_dreamfusion_2022,
	title = {{DreamFusion}: {Text}-to-{3D} using {2D} {Diffusion}},
	shorttitle = {{DreamFusion}},
	url = {http://arxiv.org/abs/2209.14988},
	doi = {10.48550/arXiv.2209.14988},
	abstract = {Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
	month = sep,
	year = {2022},
	note = {arXiv:2209.14988 [cs, stat]},
}

@inproceedings{zhang_crossformer_2022,
	title = {Crossformer: {Transformer} {Utilizing} {Cross}-{Dimension} {Dependency} for {Multivariate} {Time} {Series} {Forecasting}},
	shorttitle = {Crossformer},
	url = {https://openreview.net/forum?id=vSVLM2j9eie},
	abstract = {Recently many deep models have been proposed for multivariate time series (MTS) forecasting. In particular, Transformer-based models have shown great potential because they can capture long-term dependency. However, existing Transformer-based models mainly focus on modeling the temporal dependency (cross-time dependency) yet often omit the dependency among different variables (cross-dimension dependency), which is critical for MTS forecasting. To fill the gap, we propose Crossformer, a Transformer-based model utilizing cross-dimension dependency for MTS forecasting. In Crossformer, the input MTS is embedded into a 2D vector array through the Dimension-Segment-Wise (DSW) embedding to preserve time and dimension information. Then the Two-Stage Attention (TSA) layer is proposed to efficiently capture the cross-time and cross-dimension dependency. Utilizing DSW embedding and TSA layer, Crossformer establishes a Hierarchical Encoder-Decoder (HED) to use the information at different scales for the final forecasting. Extensive experimental results on six real-world datasets show the effectiveness of Crossformer against previous state-of-the-arts.},
	language = {en},
	urldate = {2023-11-22},
	author = {Zhang, Yunhao and Yan, Junchi},
	month = sep,
	year = {2022},
}

@misc{ho_imagen_2022,
	title = {Imagen {Video}: {High} {Definition} {Video} {Generation} with {Diffusion} {Models}},
	shorttitle = {Imagen {Video}},
	url = {http://arxiv.org/abs/2210.02303},
	doi = {10.48550/arXiv.2210.02303},
	abstract = {We present Imagen Video, a text-conditional video generation system based on a cascade of video diffusion models. Given a text prompt, Imagen Video generates high definition videos using a base video generation model and a sequence of interleaved spatial and temporal video super-resolution models. We describe how we scale up the system as a high definition text-to-video model including design decisions such as the choice of fully-convolutional temporal and spatial super-resolution models at certain resolutions, and the choice of the v-parameterization of diffusion models. In addition, we confirm and transfer findings from previous work on diffusion-based image generation to the video generation setting. Finally, we apply progressive distillation to our video models with classifier-free guidance for fast, high quality sampling. We find Imagen Video not only capable of generating videos of high fidelity, but also having a high degree of controllability and world knowledge, including the ability to generate diverse videos and text animations in various artistic styles and with 3D object understanding. See https://imagen.research.google/video/ for samples.},
	urldate = {2023-10-16},
	publisher = {arXiv},
	author = {Ho, Jonathan and Chan, William and Saharia, Chitwan and Whang, Jay and Gao, Ruiqi and Gritsenko, Alexey and Kingma, Diederik P. and Poole, Ben and Norouzi, Mohammad and Fleet, David J. and Salimans, Tim},
	month = oct,
	year = {2022},
	note = {arXiv:2210.02303 [cs]},
	keywords = {video},
}

@misc{song_denoising_2022,
	title = {Denoising {Diffusion} {Implicit} {Models}},
	url = {http://arxiv.org/abs/2010.02502},
	doi = {10.48550/arXiv.2010.02502},
	abstract = {Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples \$10 {\textbackslash}times\$ to \$50 {\textbackslash}times\$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
	month = oct,
	year = {2022},
	note = {arXiv:2010.02502 [cs]},
	keywords = {improved\_diffusion},
}

@misc{karras_elucidating_2022,
	title = {Elucidating the {Design} {Space} of {Diffusion}-{Based} {Generative} {Models}},
	url = {http://arxiv.org/abs/2206.00364},
	doi = {10.48550/arXiv.2206.00364},
	abstract = {We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.},
	urldate = {2023-11-06},
	publisher = {arXiv},
	author = {Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
	month = oct,
	year = {2022},
	note = {arXiv:2206.00364 [cs, stat]},
}

@inproceedings{wang_end--end_2022,
	title = {End-to-{End} {Modeling} of {Hierarchical} {Time} {Series} {Using} {Autoregressive} {Transformer} and {Conditional} {Normalizing} {Flow}-based {Reconciliation}},
	url = {https://ieeexplore.ieee.org/document/10031097},
	doi = {10.1109/ICDMW58026.2022.00141},
	abstract = {Multivariate time series forecasting with hierarchi-cal structure is pervasive in real-world applications, demanding not only predicting each level of the hierarchy, but also recon-ciling all forecasts to ensure coherency, i.e., the forecasts should satisfy the hierarchical aggregation constraints. Moreover, the disparities of statistical characteristics between levels can be huge, worsened by non-Gaussian distributions and non-linear correlations. To this extent, we propose a novel end-to-end hierarchical time series forecasting model, based on conditioned normalizing flow-based autoregressive transformer reconciliation, to represent complex data distribution while simultaneously reconciling the forecasts to ensure coherency. Unlike other state-of-the-art methods, we achieve the forecasting and reconciliation simultaneously without requiring any explicit post-processing step. In addition, by harnessing the power of deep model, we do not rely on any assumption such as unbiased estimates or Gaussian distribution. Our evaluation experiments are conducted on four real-world hierarchical datasets from different industrial domains (three public ones and a dataset from the application servers of Alipay11Alipay is the world's leading company in payment technology. https:/len.wikipedia.org/wiki/Alipay) and the preliminary results demonstrate efficacy of our proposed method.},
	urldate = {2023-10-04},
	booktitle = {2022 {IEEE} {International} {Conference} on {Data} {Mining} {Workshops} ({ICDMW})},
	author = {Wang, Shiyu and Zhou, Fan and Sun, Yinbo and Ma, Lintao and Zhang, James and Zheng, Yangfei},
	month = nov,
	year = {2022},
	note = {0 citations (Crossref) [2023-10-17]
ISSN: 2375-9259},
	pages = {1087--1094},
}

@inproceedings{wang_end--end_2022-1,
	title = {End-to-{End} {Modeling} of {Hierarchical} {Time} {Series} {Using} {Autoregressive} {Transformer} and {Conditional} {Normalizing} {Flow}-based {Reconciliation}},
	url = {https://ieeexplore.ieee.org/document/10031097},
	doi = {10.1109/ICDMW58026.2022.00141},
	abstract = {Multivariate time series forecasting with hierarchi-cal structure is pervasive in real-world applications, demanding not only predicting each level of the hierarchy, but also recon-ciling all forecasts to ensure coherency, i.e., the forecasts should satisfy the hierarchical aggregation constraints. Moreover, the disparities of statistical characteristics between levels can be huge, worsened by non-Gaussian distributions and non-linear correlations. To this extent, we propose a novel end-to-end hierarchical time series forecasting model, based on conditioned normalizing flow-based autoregressive transformer reconciliation, to represent complex data distribution while simultaneously reconciling the forecasts to ensure coherency. Unlike other state-of-the-art methods, we achieve the forecasting and reconciliation simultaneously without requiring any explicit post-processing step. In addition, by harnessing the power of deep model, we do not rely on any assumption such as unbiased estimates or Gaussian distribution. Our evaluation experiments are conducted on four real-world hierarchical datasets from different industrial domains (three public ones and a dataset from the application servers of Alipay11Alipay is the world's leading company in payment technology. https:/len.wikipedia.org/wiki/Alipay) and the preliminary results demonstrate efficacy of our proposed method.},
	urldate = {2023-10-05},
	booktitle = {2022 {IEEE} {International} {Conference} on {Data} {Mining} {Workshops} ({ICDMW})},
	author = {Wang, Shiyu and Zhou, Fan and Sun, Yinbo and Ma, Lintao and Zhang, James and Zheng, Yangfei},
	month = nov,
	year = {2022},
	note = {ISSN: 2375-9259},
	pages = {1087--1094},
}

@misc{xu_poisson_2022,
	title = {Poisson {Flow} {Generative} {Models}},
	url = {http://arxiv.org/abs/2209.11178},
	doi = {10.48550/arXiv.2209.11178},
	abstract = {We propose a new "Poisson flow" generative model (PFGM) that maps a uniform distribution on a high-dimensional hemisphere into any data distribution. We interpret the data points as electrical charges on the \$z=0\$ hyperplane in a space augmented with an additional dimension \$z\$, generating a high-dimensional electric field (the gradient of the solution to Poisson equation). We prove that if these charges flow upward along electric field lines, their initial distribution in the \$z=0\$ plane transforms into a distribution on the hemisphere of radius \$r\$ that becomes uniform in the \$r {\textbackslash}to{\textbackslash}infty\$ limit. To learn the bijective transformation, we estimate the normalized field in the augmented space. For sampling, we devise a backward ODE that is anchored by the physically meaningful additional dimension: the samples hit the unaugmented data manifold when the \$z\$ reaches zero. Experimentally, PFGM achieves current state-of-the-art performance among the normalizing flow models on CIFAR-10, with an Inception score of \$9.68\$ and a FID score of \$2.35\$. It also performs on par with the state-of-the-art SDE approaches while offering \$10{\textbackslash}times \$ to \$20 {\textbackslash}times\$ acceleration on image generation tasks. Additionally, PFGM appears more tolerant of estimation errors on a weaker network architecture and robust to the step size in the Euler method. The code is available at https://github.com/Newbeeer/poisson\_flow .},
	urldate = {2023-11-06},
	publisher = {arXiv},
	author = {Xu, Yilun and Liu, Ziming and Tegmark, Max and Jaakkola, Tommi},
	month = oct,
	year = {2022},
	note = {arXiv:2209.11178 [cs]},
}

@misc{bilos_modeling_2022,
	title = {Modeling {Temporal} {Data} as {Continuous} {Functions} with {Stochastic} {Process} {Diffusion}},
	url = {http://arxiv.org/abs/2211.02590},
	doi = {10.48550/arXiv.2211.02590},
	abstract = {Temporal data such as time series can be viewed as discretized measurements of the underlying function. To build a generative model for such data we have to model the stochastic process that governs it. We propose a solution by defining the denoising diffusion model in the function space which also allows us to naturally handle irregularly-sampled observations. The forward process gradually adds noise to functions, preserving their continuity, while the learned reverse process removes the noise and returns functions as new samples. To this end, we define suitable noise sources and introduce novel denoising and score-matching models. We show how our method can be used for multivariate probabilistic forecasting and imputation, and how our model can be interpreted as a neural process.},
	urldate = {2023-10-24},
	publisher = {arXiv},
	author = {Biloš, Marin and Rasul, Kashif and Schneider, Anderson and Nevmyvaka, Yuriy and Günnemann, Stephan},
	month = nov,
	year = {2022},
	note = {arXiv:2211.02590 [cs]
version: 1},
}

@misc{yang_diffusion_2022,
	title = {Diffusion {Probabilistic} {Modeling} for {Video} {Generation}},
	url = {http://arxiv.org/abs/2203.09481},
	doi = {10.48550/arXiv.2203.09481},
	abstract = {Denoising diffusion probabilistic models are a promising new class of generative models that mark a milestone in high-quality image generation. This paper showcases their ability to sequentially generate video, surpassing prior methods in perceptual and probabilistic forecasting metrics. We propose an autoregressive, end-to-end optimized video diffusion model inspired by recent advances in neural video compression. The model successively generates future frames by correcting a deterministic next-frame prediction using a stochastic residual generated by an inverse diffusion process. We compare this approach against five baselines on four datasets involving natural and simulation-based videos. We find significant improvements in terms of perceptual quality for all datasets. Furthermore, by introducing a scalable version of the Continuous Ranked Probability Score (CRPS) applicable to video, we show that our model also outperforms existing approaches in their probabilistic frame forecasting ability.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Yang, Ruihan and Srivastava, Prakhar and Mandt, Stephan},
	month = dec,
	year = {2022},
	note = {arXiv:2203.09481 [cs, stat]},
	keywords = {video},
}

@misc{harvey_flexible_2022,
	title = {Flexible {Diffusion} {Modeling} of {Long} {Videos}},
	url = {http://arxiv.org/abs/2205.11495},
	doi = {10.48550/arXiv.2205.11495},
	abstract = {We present a framework for video modeling based on denoising diffusion probabilistic models that produces long-duration video completions in a variety of realistic environments. We introduce a generative model that can at test-time sample any arbitrary subset of video frames conditioned on any other subset and present an architecture adapted for this purpose. Doing so allows us to efficiently compare and optimize a variety of schedules for the order in which frames in a long video are sampled and use selective sparse and long-range conditioning on previously sampled frames. We demonstrate improved video modeling over prior work on a number of datasets and sample temporally coherent videos over 25 minutes in length. We additionally release a new video modeling dataset and semantically meaningful metrics based on videos generated in the CARLA autonomous driving simulator.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Harvey, William and Naderiparizi, Saeid and Masrani, Vaden and Weilbach, Christian and Wood, Frank},
	month = dec,
	year = {2022},
	note = {arXiv:2205.11495 [cs]},
	keywords = {video},
}

@article{bond-taylor_deep_2022,
	title = {Deep {Generative} {Modelling}: {A} {Comparative} {Review} of {VAEs}, {GANs}, {Normalizing} {Flows}, {Energy}-{Based} and {Autoregressive} {Models}},
	volume = {44},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Deep {Generative} {Modelling}},
	url = {https://ieeexplore.ieee.org/document/9555209/},
	doi = {10.1109/TPAMI.2021.3116668},
	abstract = {Deep generative models are a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which make trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing ﬂows, in addition to numerous hybrid approaches. These techniques are compared and contrasted, explaining the premises behind each and how they are interrelated, while reviewing current state-of-the-art advances and implementations.},
	language = {en},
	number = {11},
	urldate = {2023-11-07},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bond-Taylor, Sam and Leach, Adam and Long, Yang and Willcocks, Chris G.},
	month = nov,
	year = {2022},
	pages = {7327--7347},
}

@article{baidoo-anu_education_2023,
	title = {Education in the {Era} of {Generative} {Artificial} {Intelligence} ({AI}): {Understanding} the {Potential} {Benefits} of {ChatGPT} in {Promoting} {Teaching} and {Learning}},
	issn = {1556-5068},
	shorttitle = {Education in the {Era} of {Generative} {Artificial} {Intelligence} ({AI})},
	url = {https://www.ssrn.com/abstract=4337484},
	doi = {10.2139/ssrn.4337484},
	abstract = {Since its maiden release into the public domain on November 30, 2022, ChatGPT garnered more than one million subscribers within a week. The generative AI tool ⎼ChatGPT took the world by surprise with it sophisticated capacity to carry out remarkably complex tasks. The extraordinary abilities of ChatGPT to perform complex tasks within the field of education has caused mixed feelings among educators, as this advancement in AI seems to revolutionize existing educational praxis. This is an exploratory study that synthesizes recent extant literature to offer some potential benefits and drawbacks of ChatGPT in promoting teaching and learning. Benefits of ChatGPT include but are not limited to promotion of personalized and interactive learning, generating prompts for formative assessment activities that provide ongoing feedback to inform teaching and learning etc. The paper also highlights some inherent limitations in the ChatGPT such as generating wrong information, biases in data training, which may augment existing biases, privacy issues etc. The study offers recommendations on how ChatGPT could be leveraged to maximize teaching and learning. Policy makers, researchers, educators and technology experts could work together and start conversations on how these evolving generative AI tools could be used safely and constructively to improve education and support students’ learning.},
	language = {en},
	urldate = {2023-11-07},
	journal = {SSRN Electronic Journal},
	author = {Baidoo-Anu, David and Owusu Ansah, Leticia},
	year = {2023},
}

@article{feng_multi-scale_2023,
	title = {Multi-scale {Attention} {Flow} for {Probabilistic} {Time} {Series} {Forecasting}},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/abstract/document/10265130},
	doi = {10.1109/TKDE.2023.3319672},
	abstract = {The probability prediction of multivariate time series is a notoriously challenging but practical task. On the one hand, the challenge is how to effectively capture the cross-series correlations between interacting time series, to achieve accurate distribution modeling. On the other hand, we should consider how to capture the contextual information within time series more accurately to model multivariate temporal dynamics of time series. In this work, we proposed a novel non-autoregressive deep learning model, called Multi-scale Attention Normalizing Flow(MANF), where we combine multi-scale attention with relative position information and the multivariate data distribution is represented by the conditioned normalizing flow. Additionally, compared with autoregressive modeling methods, our model avoids the influence of cumulative error and does not increase the time complexity. Extensive experiments demonstrate that our model achieves state-of-the-art performance on many popular multivariate datasets.},
	urldate = {2023-10-05},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Feng, Shibo and Miao, Chunyan and Xu, Ke and Wu, Jiaxiang and Wu, Pengcheng and Zhang, Yang and Zhao, Peilin},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	pages = {1--14},
}

@inproceedings{li_graph_2023,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Graph {Convolution} {Recurrent} {Denoising} {Diffusion} {Model} for {Multivariate} {Probabilistic} {Temporal} {Forecasting}},
	isbn = {978-3-031-46661-8},
	doi = {10.1007/978-3-031-46661-8_44},
	abstract = {The probabilistic estimation for multivariate time series forecasting has recently become a trend in various research fields, such as traffic, climate, and finance. The multivariate time series can be treated as an interrelated system, and it is significant to assume each variable to be independent. However, most existing methods fail to simultaneously consider spatial dependencies and probabilistic temporal dynamics. To address this gap, we introduce the Graph Convolution Recurrent Denoising Diffusion model (GCRDD), a recurrent framework for spatial-temporal forecasting that captures both spatial dependencies and temporal dynamics. Specifically, GCRDD incorporates the structural dependency into a hidden state using the graph-modified gated recurrent unit and samples from the estimated data distribution at each time step by a graph conditional diffusion model. We reveal the comparative experiment performance of state-of-the-art models in two real-world road network traffic datasets to demonstrate it as the competitive probabilistic multivariate temporal forecasting framework.},
	language = {en},
	booktitle = {Advanced {Data} {Mining} and {Applications}},
	publisher = {Springer Nature Switzerland},
	author = {Li, Ruikun and Li, Xuliang and Gao, Shiying and Choy, S. T. Boris and Gao, Junbin},
	editor = {Yang, Xiaochun and Suhartanto, Heru and Wang, Guoren and Wang, Bin and Jiang, Jing and Li, Bing and Zhu, Huaijie and Cui, Ningning},
	year = {2023},
	pages = {661--676},
}

@article{brophy_generative_2023,
	title = {Generative {Adversarial} {Networks} in {Time} {Series}: {A} {Systematic} {Literature} {Review}},
	volume = {55},
	issn = {0360-0300},
	shorttitle = {Generative {Adversarial} {Networks} in {Time} {Series}},
	url = {https://dl.acm.org/doi/10.1145/3559540},
	doi = {10.1145/3559540},
	abstract = {Generative adversarial network (GAN) studies have grown exponentially in the past few years. Their impact has been seen mainly in the computer vision field with realistic image and video manipulation, especially generation, making significant advancements. Although these computer vision advances have garnered much attention, GAN applications have diversified across disciplines such as time series and sequence generation. As a relatively new niche for GANs, fieldwork is ongoing to develop high-quality, diverse, and private time series data. In this article, we review GAN variants designed for time series related applications. We propose a classification of discrete-variant GANs and continuous-variant GANs, in which GANs deal with discrete time series and continuous time series data. Here we showcase the latest and most popular literature in this field—their architectures, results, and applications. We also provide a list of the most popular evaluation metrics and their suitability across applications. Also presented is a discussion of privacy measures for these GANs and further protections and directions for dealing with sensitive data. We aim to frame clearly and concisely the latest and state-of-the-art research in this area and their applications to real-world technologies.},
	number = {10},
	urldate = {2023-11-22},
	journal = {ACM Computing Surveys},
	author = {Brophy, Eoin and Wang, Zhengwei and She, Qi and Ward, Tomás},
	month = feb,
	year = {2023},
	keywords = {GAN, general time-series},
	pages = {199:1--199:31},
}

@article{yang_diffsound_2023,
	title = {Diffsound: {Discrete} {Diffusion} {Model} for {Text}-to-{Sound} {Generation}},
	volume = {31},
	issn = {2329-9304},
	shorttitle = {Diffsound},
	url = {https://ieeexplore.ieee.org/abstract/document/10112585},
	doi = {10.1109/TASLP.2023.3268730},
	abstract = {Generating sound effects that people want is an important topic. However, there are limited studies in this area for sound generation. In this study, we investigate generating sound conditioned on a text prompt and propose a novel text-to-sound generation framework that consists of a text encoder, a Vector Quantized Variational Autoencoder (VQ-VAE), a token-decoder, and a vocoder. The framework first uses the token-decoder to transfer the text features extracted from the text encoder to a mel-spectrogram with the help of VQ-VAE, and then the vocoder is used to transform the generated mel-spectrogram into a waveform. We found that the token-decoder significantly influences the generation performance. Thus, we focus on designing a good token-decoder in this study. We begin with the traditional autoregressive (AR) token-decoder. However, the AR token-decoder always predicts the mel-spectrogram tokens one by one in order, which may introduce the unidirectional bias and accumulation of errors problems. Moreover, with the AR token-decoder, the sound generation time increases linearly with the sound duration. To overcome the shortcomings introduced by AR token-decoders, we propose a non-autoregressive token-decoder based on the discrete diffusion model, named Diffsound. Specifically, the Diffsound model predicts all of the mel-spectrogram tokens in one step and then refines the predicted tokens in the next step, so the best-predicted results can be obtained by iteration. Our experiments show that our proposed Diffsound model not only produces better generation results when compared with the AR token-decoder but also has a faster generation speed, i.e., MOS: 3.56 v.s 2.786.},
	urldate = {2023-10-16},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Yang, Dongchao and Yu, Jianwei and Wang, Helin and Wang, Wen and Weng, Chao and Zou, Yuexian and Yu, Dong},
	year = {2023},
	note = {3 citations (Crossref) [2023-10-17]
Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	keywords = {speech},
	pages = {1720--1733},
}

@article{han_enhancing_2023,
	title = {Enhancing {Remote} {Sensing} {Image} {Super}-{Resolution} with {Efficient} {Hybrid} {Conditional} {Diffusion} {Model}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/15/13/3452},
	doi = {10.3390/rs15133452},
	abstract = {Recently, optical remote-sensing images have been widely applied in fields such as environmental monitoring and land cover classification. However, due to limitations in imaging equipment and other factors, low-resolution images that are unfavorable for image analysis are often obtained. Although existing image super-resolution algorithms can enhance image resolution, these algorithms are not specifically designed for the characteristics of remote-sensing images and cannot effectively recover high-resolution images. Therefore, this paper proposes a novel remote-sensing image super-resolution algorithm based on an efficient hybrid conditional diffusion model (EHC-DMSR). The algorithm applies the theory of diffusion models to remote-sensing image super-resolution. Firstly, the comprehensive features of low-resolution images are extracted through a transformer network and CNN to serve as conditions for guiding image generation. Furthermore, to constrain the diffusion model and generate more high-frequency information, a Fourier high-frequency spatial constraint is proposed to emphasize high-frequency spatial loss and optimize the reverse diffusion direction. To address the time-consuming issue of the diffusion model during the reverse diffusion process, a feature-distillation-based method is proposed to reduce the computational load of U-Net, thereby shortening the inference time without affecting the super-resolution performance. Extensive experiments on multiple test datasets demonstrated that our proposed algorithm not only achieves excellent results in quantitative evaluation metrics but also generates sharper super-resolved images with rich detailed information.},
	language = {en},
	number = {13},
	urldate = {2023-11-08},
	journal = {Remote Sensing},
	author = {Han, Lintao and Zhao, Yuchen and Lv, Hengyi and Zhang, Yisa and Liu, Hailong and Bi, Guoling and Han, Qing},
	month = jan,
	year = {2023},
	note = {0 citations (Crossref) [2023-11-08]
Number: 13
Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {3452},
}

@misc{holzschuh_score_2023,
	title = {Score {Matching} via {Differentiable} {Physics}},
	url = {http://arxiv.org/abs/2301.10250},
	abstract = {Diffusion models based on stochastic differential equations (SDEs) gradually perturb a data distribution \$p({\textbackslash}mathbf\{x\})\$ over time by adding noise to it. A neural network is trained to approximate the score \${\textbackslash}nabla\_{\textbackslash}mathbf\{x\} {\textbackslash}log p\_t({\textbackslash}mathbf\{x\})\$ at time \$t\$, which can be used to reverse the corruption process. In this paper, we focus on learning the score field that is associated with the time evolution according to a physics operator in the presence of natural non-deterministic physical processes like diffusion. A decisive difference to previous methods is that the SDE underlying our approach transforms the state of a physical system to another state at a later time. For that purpose, we replace the drift of the underlying SDE formulation with a differentiable simulator or a neural network approximation of the physics. We propose different training strategies based on the so-called probability flow ODE to fit a training set of simulation trajectories and discuss their relation to the score matching objective. For inference, we sample plausible trajectories that evolve towards a given end state using the reverse-time SDE and demonstrate the competitiveness of our approach for different challenging inverse problems.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Holzschuh, Benjamin J. and Vegetti, Simona and Thuerey, Nils},
	month = jan,
	year = {2023},
	note = {arXiv:2301.10250 [physics]},
}

@misc{murray_state---art_2023,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {State-of-{The}-{Art} {Deep} {Learning} {Models} are {Superior} for {Time} {Series} {Forecasting} and are {Applied} {Optimally} with {Iterative} {Prediction} {Methods}},
	url = {https://papers.ssrn.com/abstract=4361707},
	doi = {10.2139/ssrn.4361707},
	abstract = {In recent years many new algorithms have been developed for applications in speech and image processing which may be repurposed for time series prediction. This paper presents a comprehensive comparative analysis of time series forecasting capabilities of eight such state-of-the-art algorithms – namely: Vanilla Long Short-Term Memory(V-LSTM) Gated Recurrent Unit (GRU), Bidirectional LSTM(BD-LSTM), Auto encoder (AE-LSTM), Convolutional Neural Network LSTM(CNN-LSTM), LSTM with convolutional encoder (ConvLSTM), Attention mechanism networks and the Transformer networks.Model performances across ten different benchmark datasets including fields of interests such as finance, weather and sales are evaluated. Direct and iterative prediction methods of forecasting are also comprehensively evaluated. For comprehensive and efficient model optimization, the asynchronous successive halving algorithm (ASHA) is applied in the training folds in a 10 fold cross validation framework. Statistical tests are used to comprehensively compare algorithm performances within and across datasets.We show that whilst there are differences between all models, the differences are insignificant for the top performing models which include the Transformer, Attention, V-LSTM, CNN-LSTM and CV-LSTM. However, the transformer model consistently produces the lowest prediction error. We also show that the iterative multistep ahead prediction method is optimal for long range prediction with these new algorithms.},
	language = {en},
	urldate = {2023-11-22},
	author = {Murray, Cathal John and Du Bois, Naomi and Hollywood, Lynsey and Coyle, Damien},
	month = feb,
	year = {2023},
	keywords = {general time-series},
}

@misc{bansal_universal_2023,
	title = {Universal {Guidance} for {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2302.07121},
	doi = {10.48550/arXiv.2302.07121},
	abstract = {Typical diffusion models are trained to accept a particular form of conditioning, most commonly text, and cannot be conditioned on other modalities without retraining. In this work, we propose a universal guidance algorithm that enables diffusion models to be controlled by arbitrary guidance modalities without the need to retrain any use-specific components. We show that our algorithm successfully generates quality images with guidance functions including segmentation, face recognition, object detection, and classifier signals. Code is available at https://github.com/arpitbansal297/Universal-Guided-Diffusion.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Bansal, Arpit and Chu, Hong-Min and Schwarzschild, Avi and Sengupta, Soumyadip and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
	month = feb,
	year = {2023},
	note = {arXiv:2302.07121 [cs]},
	keywords = {explain},
}

@misc{gong_diffuseq_2023,
	title = {{DiffuSeq}: {Sequence} to {Sequence} {Text} {Generation} with {Diffusion} {Models}},
	shorttitle = {{DiffuSeq}},
	url = {http://arxiv.org/abs/2210.08933},
	doi = {10.48550/arXiv.2210.08933},
	abstract = {Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSeq is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between DiffuSeq and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks. Code is available at {\textbackslash}url\{https://github.com/Shark-NLP/DiffuSeq\}},
	urldate = {2023-10-16},
	publisher = {arXiv},
	author = {Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, Lingpeng},
	month = feb,
	year = {2023},
	note = {arXiv:2210.08933 [cs]},
	keywords = {text},
}

@misc{nie_time_2023,
	title = {A {Time} {Series} is {Worth} 64 {Words}: {Long}-term {Forecasting} with {Transformers}},
	shorttitle = {A {Time} {Series} is {Worth} 64 {Words}},
	url = {http://arxiv.org/abs/2211.14730},
	doi = {10.48550/arXiv.2211.14730},
	abstract = {We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-trained representation on one dataset to others also produces SOTA forecasting accuracy. Code is available at: https://github.com/yuqinie98/PatchTST.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Nie, Yuqi and Nguyen, Nam H. and Sinthong, Phanwadee and Kalagnanam, Jayant},
	month = mar,
	year = {2023},
	note = {arXiv:2211.14730 [cs]},
}

@misc{chang_tdstf_2023,
	title = {{TDSTF}: {Transformer}-based {Diffusion} probabilistic model for {Sparse} {Time} series {Forecasting}},
	shorttitle = {{TDSTF}},
	url = {http://arxiv.org/abs/2301.06625},
	abstract = {Background and objective: In the intensive care unit (ICU), vital sign monitoring is critical, and an accurate predictive system is required. This study will create a novel model to forecast Heart Rate (HR), Systolic Blood Pressure (SBP), and Diastolic Blood Pressure (DBP) in ICU. These vital signs are crucial for prompt interventions for patients. We extracted \$24,886\$ ICU stays from the MIMIC-III database, which contains data from over \$46\$ thousand patients, to train and test the model. Methods: The model proposed in this study, areansformerin intensive careabilistic Model for Sparse Time Series Forecasting (TDSTF), uses a deep learning technique called the Transformer. The TDSTF model showed state-of-the-art performance in predicting vital signs in the ICU, outperforming other models' ability to predict distributions of vital signs and being more computationally efficient. The code is available at https://github.com/PingChang818/TDSTF. Results: The results of the study showed that TDSTF achieved a Normalized Average Continuous Ranked Probability Score (NACRPS) of \$0.4438\$ and a Mean Squared Error (MSE) of \$0.4168\$, an improvement of \$18.9{\textbackslash}\%\$ and \$34.3{\textbackslash}\%\$ over the best baseline model, respectively. Conclusion: In conclusion, TDSTF is an effective and efficient solution for forecasting vital signs in the ICU, and it shows a significant improvement compared to other models in the field.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Chang, Ping and Li, Huayu and Quan, Stuart F. and Lu, Shuyang and Wung, Shu-Fen and Roveda, Janet and Li, Ao},
	month = mar,
	year = {2023},
	note = {arXiv:2301.06625 [cs]},
	keywords = {Diffusion, TDSTF, Transformer},
}

@misc{yang_diffusion_2023,
	title = {Diffusion {Models}: {A} {Comprehensive} {Survey} of {Methods} and {Applications}},
	shorttitle = {Diffusion {Models}},
	url = {http://arxiv.org/abs/2209.00796},
	abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
	month = mar,
	year = {2023},
	note = {arXiv:2209.00796 [cs]},
	keywords = {Diffusion},
}

@misc{brynjolfsson_generative_2023,
	type = {Working {Paper}},
	series = {Working {Paper} {Series}},
	title = {Generative {AI} at {Work}},
	url = {https://www.nber.org/papers/w31161},
	doi = {10.3386/w31161},
	abstract = {New AI tools have the potential to change the way workers perform and learn, but little is known about their impacts on the job. In this paper, we study the staggered introduction of a generative AI-based conversational assistant using data from 5,179 customer support agents. Access to the tool increases productivity, as measured by issues resolved per hour, by 14\% on average, including a 34\% improvement for novice and low-skilled workers but with minimal impact on experienced and highly skilled workers. We provide suggestive evidence that the AI model disseminates the best practices of more able workers and helps newer workers move down the experience curve. In addition, we find that AI assistance improves customer sentiment, increases employee retention, and may lead to worker learning. Our results suggest that access to generative AI can increase productivity, with large heterogeneity in effects across workers.},
	urldate = {2023-11-07},
	publisher = {National Bureau of Economic Research},
	author = {Brynjolfsson, Erik and Li, Danielle and Raymond, Lindsey R.},
	month = apr,
	year = {2023},
	doi = {10.3386/w31161},
}

@misc{zhang_effectively_2023,
	title = {Effectively {Modeling} {Time} {Series} with {Simple} {Discrete} {State} {Spaces}},
	url = {http://arxiv.org/abs/2303.09489},
	abstract = {Time series modeling is a well-established problem, which often requires that methods (1) expressively represent complicated dependencies, (2) forecast long horizons, and (3) efficiently train over long sequences. State-space models (SSMs) are classical models for time series, and prior works combine SSMs with deep learning layers for efficient sequence modeling. However, we find fundamental limitations with these prior approaches, proving their SSM representations cannot express autoregressive time series processes. We thus introduce SpaceTime, a new state-space time series architecture that improves all three criteria. For expressivity, we propose a new SSM parameterization based on the companion matrix -- a canonical representation for discrete-time processes -- which enables SpaceTime's SSM layers to learn desirable autoregressive processes. For long horizon forecasting, we introduce a "closed-loop" variation of the companion SSM, which enables SpaceTime to predict many future time-steps by generating its own layer-wise inputs. For efficient training and inference, we introduce an algorithm that reduces the memory and compute of a forward pass with the companion matrix. With sequence length \${\textbackslash}ell\$ and state-space size \$d\$, we go from \${\textbackslash}tilde\{O\}(d {\textbackslash}ell)\$ na{\textbackslash}"ively to \${\textbackslash}tilde\{O\}(d + {\textbackslash}ell)\$. In experiments, our contributions lead to state-of-the-art results on extensive and diverse benchmarks, with best or second-best AUROC on 6 / 7 ECG and speech time series classification, and best MSE on 14 / 16 Informer forecasting tasks. Furthermore, we find SpaceTime (1) fits AR(\$p\$) processes that prior deep SSMs fail on, (2) forecasts notably more accurately on longer horizons than prior state-of-the-art, and (3) speeds up training on real-world ETTh1 data by 73\% and 80\% relative wall-clock time over Transformers and LSTMs.},
	urldate = {2023-10-04},
	publisher = {arXiv},
	author = {Zhang, Michael and Saab, Khaled K. and Poli, Michael and Dao, Tri and Goel, Karan and Ré, Christopher},
	month = mar,
	year = {2023},
	note = {arXiv:2303.09489 [cs]},
}

@misc{aggarwal_embarrassingly_2023,
	title = {Embarrassingly {Simple} {MixUp} for {Time}-series},
	url = {http://arxiv.org/abs/2304.04271},
	doi = {10.48550/arXiv.2304.04271},
	abstract = {Labeling time series data is an expensive task because of domain expertise and dynamic nature of the data. Hence, we often have to deal with limited labeled data settings. Data augmentation techniques have been successfully deployed in domains like computer vision to exploit the use of existing labeled data. We adapt one of the most commonly used technique called MixUp, in the time series domain. Our proposed, MixUp++ and LatentMixUp++, use simple modifications to perform interpolation in raw time series and classification model's latent space, respectively. We also extend these methods with semi-supervised learning to exploit unlabeled data. We observe significant improvements of 1{\textbackslash}\% - 15{\textbackslash}\% on time series classification on two public datasets, for both low labeled data as well as high labeled data regimes, with LatentMixUp++.},
	urldate = {2023-10-04},
	publisher = {arXiv},
	author = {Aggarwal, Karan and Srivastava, Jaideep},
	month = apr,
	year = {2023},
	note = {arXiv:2304.04271 [cs]},
}

@misc{lin_diffusion_2023,
	title = {Diffusion {Models} for {Time} {Series} {Applications}: {A} {Survey}},
	shorttitle = {Diffusion {Models} for {Time} {Series} {Applications}},
	url = {http://arxiv.org/abs/2305.00624},
	doi = {10.48550/arXiv.2305.00624},
	abstract = {Diffusion models, a family of generative models based on deep learning, have become increasingly prominent in cutting-edge machine learning research. With a distinguished performance in generating samples that resemble the observed data, diffusion models are widely used in image, video, and text synthesis nowadays. In recent years, the concept of diffusion has been extended to time series applications, and many powerful models have been developed. Considering the deficiency of a methodical summary and discourse on these models, we provide this survey as an elementary resource for new researchers in this area and also an inspiration to motivate future research. For better understanding, we include an introduction about the basics of diffusion models. Except for this, we primarily focus on diffusion-based methods for time series forecasting, imputation, and generation, and present them respectively in three individual sections. We also compare different methods for the same application and highlight their connections if applicable. Lastly, we conclude the common limitation of diffusion-based methods and highlight potential future research directions.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Lin, Lequan and Li, Zhengkun and Li, Ruikun and Li, Xuliang and Gao, Junbin},
	month = apr,
	year = {2023},
	note = {arXiv:2305.00624 [cs]},
	keywords = {Diffusion},
}

@inproceedings{qadir_engineering_2023,
	address = {Kuwait, Kuwait},
	title = {Engineering {Education} in the {Era} of {ChatGPT}: {Promise} and {Pitfalls} of {Generative} {AI} for {Education}},
	isbn = {9798350399431},
	shorttitle = {Engineering {Education} in the {Era} of {ChatGPT}},
	url = {https://ieeexplore.ieee.org/document/10125121/},
	doi = {10.1109/EDUCON54358.2023.10125121},
	abstract = {Engineering education is constantly evolving to keep up with the latest technological developments and meet the changing needs of the engineering industry. One promising development in this field is the use of generative artificial intelligence technology, such as the ChatGPT conversational agent. ChatGPT has the potential to offer personalized and effective learning experiences by providing students with customized feedback and explanations, as well as creating realistic virtual simulations for hands-on learning. However, it is important to also consider the limitations of this technology. ChatGPT and other generative AI systems are only as good as their training data and may perpetuate biases or even generate and spread misinformation. Additionally, the use of generative AI in education raises ethical concerns such as the potential for unethical or dishonest use by students and the potential unemployment of humans who are made redundant by technology. While the current state of generative AI technology represented by ChatGPT is impressive but flawed, it is only a preview of what is to come. It is important for engineering educators to understand the implications of this technology and study how to adapt the engineering education ecosystem to ensure that the next generation of engineers can take advantage of the benefits offered by generative AI while minimizing any negative consequences.},
	language = {en},
	urldate = {2023-11-07},
	booktitle = {2023 {IEEE} {Global} {Engineering} {Education} {Conference} ({EDUCON})},
	publisher = {IEEE},
	author = {Qadir, Junaid},
	month = may,
	year = {2023},
	pages = {1--9},
}

@misc{alcaraz_diffusion-based_2023,
	title = {Diffusion-based {Time} {Series} {Imputation} and {Forecasting} with {Structured} {State} {Space} {Models}},
	url = {http://arxiv.org/abs/2208.09399},
	doi = {10.48550/arXiv.2208.09399},
	abstract = {The imputation of missing values represents a significant obstacle for many real-world data analysis pipelines. Here, we focus on time series data and put forward SSSD, an imputation model that relies on two emerging technologies, (conditional) diffusion models as state-of-the-art generative models and structured state space models as internal model architecture, which are particularly suited to capture long-term dependencies in time series data. We demonstrate that SSSD matches or even exceeds state-of-the-art probabilistic imputation and forecasting performance on a broad range of data sets and different missingness scenarios, including the challenging blackout-missing scenarios, where prior approaches failed to provide meaningful results.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Alcaraz, Juan Miguel Lopez and Strodthoff, Nils},
	month = may,
	year = {2023},
	note = {arXiv:2208.09399 [cs, stat]},
	keywords = {Diffusion, SSSD{\textasciicircum}S4, State-Space-Model},
}

@misc{wen_transformers_2023,
	title = {Transformers in {Time} {Series}: {A} {Survey}},
	shorttitle = {Transformers in {Time} {Series}},
	url = {http://arxiv.org/abs/2202.07125},
	doi = {10.48550/arXiv.2202.07125},
	abstract = {Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also triggered great interest in the time series community. Among multiple advantages of Transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations. In particular, we examine the development of time series Transformers in two perspectives. From the perspective of network structure, we summarize the adaptations and modifications that have been made to Transformers in order to accommodate the challenges in time series analysis. From the perspective of applications, we categorize time series Transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically, we perform robust analysis, model size analysis, and seasonal-trend decomposition analysis to study how Transformers perform in time series. Finally, we discuss and suggest future directions to provide useful research guidance. To the best of our knowledge, this paper is the first work to comprehensively and systematically summarize the recent advances of Transformers for modeling time series data. We hope this survey will ignite further research interests in time series Transformers.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
	month = may,
	year = {2023},
	note = {arXiv:2202.07125 [cs, eess, stat]},
	keywords = {Transformer, general time-series},
}

@misc{wang_diffload_2023,
	title = {{DiffLoad}: {Uncertainty} {Quantification} in {Load} {Forecasting} with {Diffusion} {Model}},
	shorttitle = {{DiffLoad}},
	url = {http://arxiv.org/abs/2306.01001},
	abstract = {Electrical load forecasting is of great significance for the decision makings in power systems, such as unit commitment and energy management. In recent years, various self-supervised neural network-based methods have been applied to electrical load forecasting to improve forecasting accuracy and capture uncertainties. However, most current methods are based on Gaussian likelihood methods, which aim to accurately estimate the distribution expectation under a given covariate. This kind of approach is difficult to adapt to situations where temporal data has a distribution shift and outliers. In this paper, we propose a diffusion-based Seq2seq structure to estimate epistemic uncertainty and use the robust additive Cauchy distribution to estimate aleatoric uncertainty. Rather than accurately forecasting conditional expectations, we demonstrate our method's ability in separating two types of uncertainties and dealing with the mutant scenarios.},
	urldate = {2023-10-19},
	publisher = {arXiv},
	author = {Wang, Zhixian and Wen, Qingsong and Zhang, Chaoli and Sun, Liang and Wang, Yi},
	month = may,
	year = {2023},
	note = {arXiv:2306.01001 [cs, stat]},
}

@article{du_saits_2023,
	title = {{SAITS}: {Self}-{Attention}-based {Imputation} for {Time} {Series}},
	volume = {219},
	issn = {09574174},
	shorttitle = {{SAITS}},
	url = {http://arxiv.org/abs/2202.08516},
	doi = {10.1016/j.eswa.2023.119619},
	abstract = {Missing data in time series is a pervasive problem that puts obstacles in the way of advanced analysis. A popular solution is imputation, where the fundamental challenge is to determine what values should be filled in. This paper proposes SAITS, a novel method based on the self-attention mechanism for missing value imputation in multivariate time series. Trained by a joint-optimization approach, SAITS learns missing values from a weighted combination of two diagonally-masked self-attention (DMSA) blocks. DMSA explicitly captures both the temporal dependencies and feature correlations between time steps, which improves imputation accuracy and training speed. Meanwhile, the weighted-combination design enables SAITS to dynamically assign weights to the learned representations from two DMSA blocks according to the attention map and the missingness information. Extensive experiments quantitatively and qualitatively demonstrate that SAITS outperforms the state-of-the-art methods on the time-series imputation task efficiently and reveal SAITS' potential to improve the learning performance of pattern recognition models on incomplete time-series data from the real world. The code is open source on GitHub at https://github.com/WenjieDu/SAITS.},
	urldate = {2023-10-03},
	journal = {Expert Systems with Applications},
	author = {Du, Wenjie and Cote, David and Liu, Yan},
	month = jun,
	year = {2023},
	note = {arXiv:2202.08516 [cs]},
	pages = {119619},
}

@misc{shen_non-autoregressive_2023,
	title = {Non-autoregressive {Conditional} {Diffusion} {Models} for {Time} {Series} {Prediction}},
	url = {http://arxiv.org/abs/2306.05043},
	doi = {10.48550/arXiv.2306.05043},
	abstract = {Recently, denoising diffusion models have led to significant breakthroughs in the generation of images, audio and text. However, it is still an open question on how to adapt their strong modeling ability to model time series. In this paper, we propose TimeDiff, a non-autoregressive diffusion model that achieves high-quality time series prediction with the introduction of two novel conditioning mechanisms: future mixup and autoregressive initialization. Similar to teacher forcing, future mixup allows parts of the ground-truth future predictions for conditioning, while autoregressive initialization helps better initialize the model with basic time series patterns such as short-term trends. Extensive experiments are performed on nine real-world datasets. Results show that TimeDiff consistently outperforms existing time series diffusion models, and also achieves the best overall performance across a variety of the existing strong baselines (including transformers and FiLM).},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Shen, Lifeng and Kwok, James},
	month = jun,
	year = {2023},
	note = {arXiv:2306.05043 [cs]},
	keywords = {Diffusion, TimeDiff},
}

@misc{shu_data_2023,
	title = {Data {Augmentation} for {Seizure} {Prediction} with {Generative} {Diffusion} {Model}},
	url = {http://arxiv.org/abs/2306.08256},
	doi = {10.48550/arXiv.2306.08256},
	abstract = {Objective: Seizure prediction is of great importance to improve the life of patients. The focal point is to distinguish preictal states from interictal ones. With the development of machine learning, seizure prediction methods have achieved significant progress. However, the severe imbalance problem between preictal and interictal data still poses a great challenge, restricting the performance of classifiers. Data augmentation is an intuitive way to solve this problem. Existing data augmentation methods generate samples by overlapping or recombining data. The distribution of generated samples is limited by original data, because such transformations cannot fully explore the feature space and offer new information. As the epileptic EEG representation varies among seizures, these generated samples cannot provide enough diversity to achieve high performance on a new seizure. As a consequence, we propose a novel data augmentation method with diffusion model called DiffEEG. Methods: Diffusion models are a class of generative models that consist of two processes. Specifically, in the diffusion process, the model adds noise to the input EEG sample step by step and converts the noisy sample into output random noise, exploring the distribution of data by minimizing the loss between the output and the noise added. In the denoised process, the model samples the synthetic data by removing the noise gradually, diffusing the data distribution to outward areas and narrowing the distance between different clusters. Results: We compared DiffEEG with existing methods, and integrated them into three representative classifiers. The experiments indicate that DiffEEG could further improve the performance and shows superiority to existing methods. Conclusion: This paper proposes a novel and effective method to solve the imbalanced problem and demonstrates the effectiveness and generality of our method.},
	urldate = {2023-10-19},
	publisher = {arXiv},
	author = {Shu, Kai and Zhao, Yuchang and Wu, Le and Liu, Aiping and Qian, Ruobing and Chen, Xun},
	month = jun,
	year = {2023},
	note = {arXiv:2306.08256 [cs, eess]},
}

@article{lim_generative_2023,
	title = {Generative {AI} and the future of education: {Ragnarök} or reformation? {A} paradoxical perspective from management educators},
	volume = {21},
	issn = {1472-8117},
	shorttitle = {Generative {AI} and the future of education},
	url = {https://www.sciencedirect.com/science/article/pii/S1472811723000289},
	doi = {10.1016/j.ijme.2023.100790},
	abstract = {Generative artificial intelligence (AI) has taken the world by storm, with notable tension transpiring in the field of education. Given that Generative AI is rapidly emerging as a transformative innovation, this article endeavors to offer a seminal rejoinder that aims to (i) reconcile the great debate on Generative AI in order to (ii) lay the foundation for Generative AI to co-exist as a transformative resource in the future of education. Using critical analysis as a method and paradox theory as a theoretical lens (i.e., the “how”), this article (i) defines Generative AI and transformative education (i.e., the “ideas”), (ii) establishes the paradoxes of Generative AI (i.e., the “what”), and (iii) provides implications for the future of education from the perspective of management educators (i.e., the “so what”). Noteworthily, the paradoxes of Generative AI are four-fold: (Paradox \#1) Generative AI is a ‘friend’ yet a ‘foe’, (Paradox \#2) Generative AI is ‘capable’ yet ‘dependent’, (Paradox \#3) Generative AI is ‘accessible’ yet ‘restrictive’, and (Paradox \#4) Generative AI gets even ‘popular’ when ‘banned’ (i.e., the “what”). Through a position that seeks to embrace rather than reject Generative AI, the lessons and implications that emerge from the discussion herein represent a seminal contribution from management educators on this trending topic and should be useful for approaching Generative AI as a game-changer for education reformation in management and the field of education at large, and by extension, mitigating a situation where Generative AI develops into a Ragnarök that dooms the future of education of which management education is a part of (i.e., the “so what”).},
	number = {2},
	urldate = {2023-11-07},
	journal = {The International Journal of Management Education},
	author = {Lim, Weng Marc and Gunasekara, Asanka and Pallant, Jessica Leigh and Pallant, Jason Ian and Pechenkina, Ekaterina},
	month = jul,
	year = {2023},
	pages = {100790},
}

@misc{koo_comprehensive_2023,
	title = {A {Comprehensive} {Survey} on {Generative} {Diffusion} {Models} for {Structured} {Data}},
	url = {http://arxiv.org/abs/2306.04139},
	doi = {10.48550/arXiv.2306.04139},
	abstract = {In recent years, generative diffusion models have achieved a rapid paradigm shift in deep generative models by showing groundbreaking performance across various applications. Meanwhile, structured data, encompassing tabular and time series data, has been received comparatively limited attention from the deep learning research community, despite its omnipresence and extensive applications. Thus, there is still a lack of literature and its reviews on structured data modelling via diffusion models, compared to other data modalities such as visual and textual data. To address this gap, we present a comprehensive review of recently proposed diffusion models in the field of structured data. First, this survey provides a concise overview of the score-based diffusion model theory, subsequently proceeding to the technical descriptions of the majority of pioneering works that used structured data in both data-driven general tasks and domain-specific applications. Thereafter, we analyse and discuss the limitations and challenges shown in existing works and suggest potential research directions. We hope this review serves as a catalyst for the research community, promoting developments in generative diffusion models for structured data.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Koo, Heejoon and Kim, To Eun},
	month = jul,
	year = {2023},
	note = {arXiv:2306.04139 [cs]},
	keywords = {Diffusion},
}

@misc{kollovieh_predict_2023,
	title = {Predict, {Refine}, {Synthesize}: {Self}-{Guiding} {Diffusion} {Models} for {Probabilistic} {Time} {Series} {Forecasting}},
	shorttitle = {Predict, {Refine}, {Synthesize}},
	url = {http://arxiv.org/abs/2307.11494},
	doi = {10.48550/arXiv.2307.11494},
	abstract = {Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains. Prior works on time series diffusion models have primarily focused on developing conditional models tailored to specific forecasting or imputation tasks. In this work, we explore the potential of task-agnostic, unconditional diffusion models for several time series applications. We propose TSDiff, an unconditionally trained diffusion model for time series. Our proposed self-guidance mechanism enables conditioning TSDiff for downstream tasks during inference, without requiring auxiliary networks or altering the training procedure. We demonstrate the effectiveness of our method on three different time series tasks: forecasting, refinement, and synthetic data generation. First, we show that TSDiff is competitive with several task-specific conditional forecasting methods (predict). Second, we leverage the learned implicit probability density of TSDiff to iteratively refine the predictions of base forecasters with reduced computational overhead over reverse diffusion (refine). Notably, the generative performance of the model remains intact -- downstream forecasters trained on synthetic samples from TSDiff outperform forecasters that are trained on samples from other state-of-the-art generative time series models, occasionally even outperforming models trained on real data (synthesize).},
	urldate = {2023-10-05},
	publisher = {arXiv},
	author = {Kollovieh, Marcel and Ansari, Abdul Fatir and Bohlke-Schneider, Michael and Zschiegner, Jasper and Wang, Hao and Wang, Yuyang},
	month = jul,
	year = {2023},
	note = {arXiv:2307.11494 [cs, stat]},
	keywords = {TSDiff},
}

@article{dwivedi_opinion_2023,
	title = {Opinion {Paper}: “{So} what if {ChatGPT} wrote it?” {Multidisciplinary} perspectives on opportunities, challenges and implications of generative conversational {AI} for research, practice and policy},
	volume = {71},
	issn = {0268-4012},
	shorttitle = {Opinion {Paper}},
	url = {https://www.sciencedirect.com/science/article/pii/S0268401223000233},
	doi = {10.1016/j.ijinfomgt.2023.102642},
	abstract = {Transformative artificially intelligent tools, such as ChatGPT, designed to generate sophisticated text indistinguishable from that produced by a human, are applicable across a wide range of contexts. The technology presents opportunities as well as, often ethical and legal, challenges, and has the potential for both positive and negative impacts for organisations, society, and individuals. Offering multi-disciplinary insight into some of these, this article brings together 43 contributions from experts in fields such as computer science, marketing, information systems, education, policy, hospitality and tourism, management, publishing, and nursing. The contributors acknowledge ChatGPT’s capabilities to enhance productivity and suggest that it is likely to offer significant gains in the banking, hospitality and tourism, and information technology industries, and enhance business activities, such as management and marketing. Nevertheless, they also consider its limitations, disruptions to practices, threats to privacy and security, and consequences of biases, misuse, and misinformation. However, opinion is split on whether ChatGPT’s use should be restricted or legislated. Drawing on these contributions, the article identifies questions requiring further research across three thematic areas: knowledge, transparency, and ethics; digital transformation of organisations and societies; and teaching, learning, and scholarly research. The avenues for further research include: identifying skills, resources, and capabilities needed to handle generative AI; examining biases of generative AI attributable to training datasets and processes; exploring business and societal contexts best suited for generative AI implementation; determining optimal combinations of human and generative AI for various tasks; identifying ways to assess accuracy of text produced by generative AI; and uncovering the ethical and legal issues in using generative AI across different contexts.},
	urldate = {2023-11-07},
	journal = {International Journal of Information Management},
	author = {Dwivedi, Yogesh K. and Kshetri, Nir and Hughes, Laurie and Slade, Emma Louise and Jeyaraj, Anand and Kar, Arpan Kumar and Baabdullah, Abdullah M. and Koohang, Alex and Raghavan, Vishnupriya and Ahuja, Manju and Albanna, Hanaa and Albashrawi, Mousa Ahmad and Al-Busaidi, Adil S. and Balakrishnan, Janarthanan and Barlette, Yves and Basu, Sriparna and Bose, Indranil and Brooks, Laurence and Buhalis, Dimitrios and Carter, Lemuria and Chowdhury, Soumyadeb and Crick, Tom and Cunningham, Scott W. and Davies, Gareth H. and Davison, Robert M. and Dé, Rahul and Dennehy, Denis and Duan, Yanqing and Dubey, Rameshwar and Dwivedi, Rohita and Edwards, John S. and Flavián, Carlos and Gauld, Robin and Grover, Varun and Hu, Mei-Chih and Janssen, Marijn and Jones, Paul and Junglas, Iris and Khorana, Sangeeta and Kraus, Sascha and Larsen, Kai R. and Latreille, Paul and Laumer, Sven and Malik, F. Tegwen and Mardani, Abbas and Mariani, Marcello and Mithas, Sunil and Mogaji, Emmanuel and Nord, Jeretta Horn and O’Connor, Siobhan and Okumus, Fevzi and Pagani, Margherita and Pandey, Neeraj and Papagiannidis, Savvas and Pappas, Ilias O. and Pathak, Nishith and Pries-Heje, Jan and Raman, Ramakrishnan and Rana, Nripendra P. and Rehm, Sven-Volker and Ribeiro-Navarrete, Samuel and Richter, Alexander and Rowe, Frantz and Sarker, Suprateek and Stahl, Bernd Carsten and Tiwari, Manoj Kumar and van der Aalst, Wil and Venkatesh, Viswanath and Viglia, Giampaolo and Wade, Michael and Walton, Paul and Wirtz, Jochen and Wright, Ryan},
	month = aug,
	year = {2023},
	pages = {102642},
}

@misc{zand_diffusion_2023,
	title = {Diffusion {Models} with {Deterministic} {Normalizing} {Flow} {Priors}},
	url = {http://arxiv.org/abs/2309.01274},
	abstract = {For faster sampling and higher sample quality, we propose DiNof (\${\textbackslash}textbf\{Di\}\$ffusion with \${\textbackslash}textbf\{No\}\$rmalizing \${\textbackslash}textbf\{f\}\$low priors), a technique that makes use of normalizing flows and diffusion models. We use normalizing flows to parameterize the noisy data at any arbitrary step of the diffusion process and utilize it as the prior in the reverse diffusion process. More specifically, the forward noising process turns a data distribution into partially noisy data, which are subsequently transformed into a Gaussian distribution by a nonlinear process. The backward denoising procedure begins with a prior created by sampling from the Gaussian distribution and applying the invertible normalizing flow transformations deterministically. To generate the data distribution, the prior then undergoes the remaining diffusion stochastic denoising procedure. Through the reduction of the number of total diffusion steps, we are able to speed up both the forward and backward processes. More importantly, we improve the expressive power of diffusion models by employing both deterministic and stochastic mappings. Experiments on standard image generation datasets demonstrate the advantage of the proposed method over existing approaches. On the unconditional CIFAR10 dataset, for example, we achieve an FID of 2.01 and an Inception score of 9.96. Our method also demonstrates competitive performance on CelebA-HQ-256 dataset as it obtains an FID score of 7.11. Code is available at https://github.com/MohsenZand/DiNof.},
	urldate = {2023-10-23},
	publisher = {arXiv},
	author = {Zand, Mohsen and Etemad, Ali and Greenspan, Michael},
	month = sep,
	year = {2023},
	note = {arXiv:2309.01274 [cs]},
	keywords = {Diffusion, NF},
}

@misc{koa_diffusion_2023,
	title = {Diffusion {Variational} {Autoencoder} for {Tackling} {Stochasticity} in {Multi}-{Step} {Regression} {Stock} {Price} {Prediction}},
	url = {http://arxiv.org/abs/2309.00073},
	doi = {10.1145/3583780.3614844},
	abstract = {Multi-step stock price prediction over a long-term horizon is crucial for forecasting its volatility, allowing financial institutions to price and hedge derivatives, and banks to quantify the risk in their trading books. Additionally, most financial regulators also require a liquidity horizon of several days for institutional investors to exit their risky assets, in order to not materially affect market prices. However, the task of multi-step stock price prediction is challenging, given the highly stochastic nature of stock data. Current solutions to tackle this problem are mostly designed for single-step, classification-based predictions, and are limited to low representation expressiveness. The problem also gets progressively harder with the introduction of the target price sequence, which also contains stochastic noise and reduces generalizability at test-time. To tackle these issues, we combine a deep hierarchical variational-autoencoder (VAE) and diffusion probabilistic techniques to do seq2seq stock prediction through a stochastic generative process. The hierarchical VAE allows us to learn the complex and low-level latent variables for stock prediction, while the diffusion probabilistic model trains the predictor to handle stock price stochasticity by progressively adding random noise to the stock data. Our Diffusion-VAE (D-Va) model is shown to outperform state-of-the-art solutions in terms of its prediction accuracy and variance. More importantly, the multi-step outputs can also allow us to form a stock portfolio over the prediction length. We demonstrate the effectiveness of our model outputs in the portfolio investment task through the Sharpe ratio metric and highlight the importance of dealing with different types of prediction uncertainties.},
	urldate = {2023-10-19},
	author = {Koa, Kelvin J. L. and Ma, Yunshan and Ng, Ritchie and Chua, Tat-Seng},
	month = aug,
	year = {2023},
	note = {arXiv:2309.00073 [cs, q-fin]},
}

@article{croitoru_diffusion_2023,
	title = {Diffusion {Models} in {Vision}: {A} {Survey}},
	volume = {45},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Diffusion {Models} in {Vision}},
	url = {https://ieeexplore.ieee.org/document/10081412/},
	doi = {10.1109/TPAMI.2023.3261988},
	abstract = {Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e., low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the ﬁeld. First, we identify and present three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. We further discuss the relations between diffusion models and other deep generative models, including variational auto-encoders, generative adversarial networks, energy-based models, autoregressive models and normalizing ﬂows. Then, we introduce a multi-perspective categorization of diffusion models applied in computer vision. Finally, we illustrate the current limitations of diffusion models and envision some interesting directions for future research.},
	language = {en},
	number = {9},
	urldate = {2023-11-07},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Croitoru, Florinel-Alin and Hondru, Vlad and Ionescu, Radu Tudor and Shah, Mubarak},
	month = sep,
	year = {2023},
	pages = {10850--10869},
}

@article{cramer_multivariate_2023,
	title = {Multivariate probabilistic forecasting of intraday electricity prices using normalizing flows},
	volume = {346},
	issn = {0306-2619},
	url = {https://www.sciencedirect.com/science/article/pii/S0306261923007341},
	doi = {10.1016/j.apenergy.2023.121370},
	abstract = {Electricity is traded on various markets with different time horizons and regulations. Short-term intraday trading becomes increasingly important due to the higher penetration of renewables. In Germany, the intraday electricity price typically fluctuates around the day-ahead price of the European Power EXchange (EPEX) spot markets in a distinct hourly pattern. This work proposes a probabilistic modeling approach that models the intraday price difference to the day-ahead contracts. The model captures the emerging hourly pattern by considering the four 15min intervals in each day-ahead price interval as a four-dimensional joint probability distribution. The resulting nontrivial, multivariate price difference distribution is learned using a normalizing flow, i.e., a deep generative model that combines conditional multivariate density estimation and probabilistic regression. Furthermore, this work discusses the influence of different external impact factors based on literature insights and impact analysis using explainable artificial intelligence (XAI). The normalizing flow is compared to an informed selection of historical data and probabilistic forecasts using a Gaussian copula and a Gaussian regression model. Among the different models, the normalizing flow identifies the trends with the highest accuracy and has the narrowest prediction intervals. Both the XAI analysis and the empirical experiments highlight that the immediate history of the price difference realization and the increments of the day-ahead price have the most substantial impact on the price difference.},
	urldate = {2023-10-04},
	journal = {Applied Energy},
	author = {Cramer, Eike and Witthaut, Dirk and Mitsos, Alexander and Dahmen, Manuel},
	month = sep,
	year = {2023},
	pages = {121370},
}

@misc{yu_latent_2023,
	title = {Latent {Diffusion} {Energy}-{Based} {Model} for {Interpretable} {Text} {Modeling}},
	url = {http://arxiv.org/abs/2206.05895},
	doi = {10.48550/arXiv.2206.05895},
	abstract = {Latent space Energy-Based Models (EBMs), also known as energy-based priors, have drawn growing interests in generative modeling. Fueled by its flexibility in the formulation and strong modeling power of the latent space, recent works built upon it have made interesting attempts aiming at the interpretability of text modeling. However, latent space EBMs also inherit some flaws from EBMs in data space; the degenerate MCMC sampling quality in practice can lead to poor generation quality and instability in training, especially on data with complex latent structures. Inspired by the recent efforts that leverage diffusion recovery likelihood learning as a cure for the sampling issue, we introduce a novel symbiosis between the diffusion models and latent space EBMs in a variational learning framework, coined as the latent diffusion energy-based model. We develop a geometric clustering-based regularization jointly with the information bottleneck to further improve the quality of the learned latent space. Experiments on several challenging tasks demonstrate the superior performance of our model on interpretable text modeling over strong counterparts.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Yu, Peiyu and Xie, Sirui and Ma, Xiaojian and Jia, Baoxiong and Pang, Bo and Gao, Ruiqi and Zhu, Yixin and Zhu, Song-Chun and Wu, Ying Nian},
	month = oct,
	year = {2023},
	note = {arXiv:2206.05895 [cs]},
	keywords = {text},
}

@misc{cachay_dyffusion_2023,
	title = {{DYffusion}: {A} {Dynamics}-informed {Diffusion} {Model} for {Spatiotemporal} {Forecasting}},
	shorttitle = {{DYffusion}},
	url = {http://arxiv.org/abs/2306.01984},
	abstract = {While diffusion models can successfully generate data and make predictions, they are predominantly designed for static images. We propose an approach for efficiently training diffusion models for probabilistic spatiotemporal forecasting, where generating stable and accurate rollout forecasts remains challenging, Our method, DYffusion, leverages the temporal dynamics in the data, directly coupling it with the diffusion steps in the model. We train a stochastic, time-conditioned interpolator and a forecaster network that mimic the forward and reverse processes of standard diffusion models, respectively. DYffusion naturally facilitates multi-step and long-range forecasting, allowing for highly flexible, continuous-time sampling trajectories and the ability to trade-off performance with accelerated sampling at inference time. In addition, the dynamics-informed diffusion process in DYffusion imposes a strong inductive bias and significantly improves computational efficiency compared to traditional Gaussian noise-based diffusion models. Our approach performs competitively on probabilistic forecasting of complex dynamics in sea surface temperatures, Navier-Stokes flows, and spring mesh systems.},
	urldate = {2023-10-19},
	publisher = {arXiv},
	author = {Cachay, Salva Rühling and Zhao, Bo and Joren, Hailey and Yu, Rose},
	month = oct,
	year = {2023},
	note = {arXiv:2306.01984 [cs, stat]},
}

@misc{chang_design_2023,
	title = {On the {Design} {Fundamentals} of {Diffusion} {Models}: {A} {Survey}},
	shorttitle = {On the {Design} {Fundamentals} of {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2306.04542},
	abstract = {Diffusion models are generative models, which gradually add and remove noise to learn the underlying distribution of training data for data generation. The components of diffusion models have gained significant attention with many design choices proposed. Existing reviews have primarily focused on higher-level solutions, thereby covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review on component-wise design choices in diffusion models. Specifically, we organize this review according to their three key components, namely the forward process, the reverse process, and the sampling procedure. This allows us to provide a fine-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the applicability of design choices, and the implementation of diffusion models.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Chang, Ziyi and Koulieris, George Alex and Shum, Hubert P. H.},
	month = oct,
	year = {2023},
	note = {arXiv:2306.04542 [cs]},
}
